import os
import json
from typing import List, Dict, Any, Optional

import streamlit as st
import pandas as pd
import requests
import googlemaps
from bs4 import BeautifulSoup
from urllib.parse import urlparse

from openai import OpenAI

# =========================
# åŸºæœ¬é…ç½® & Secrets
# =========================
st.set_page_config(
    page_title="Restaurant Local SEO & Competitor Analyzer",
    layout="wide",
)

# ä» Streamlit Secrets è¯»å– API å¯†é’¥
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY", "")
SERPAPI_KEY = st.secrets.get("SERPAPI_KEY", "")
YELP_API_KEY = st.secrets.get("YELP_API_KEY", "")
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")

if not GOOGLE_API_KEY:
    st.error("ç¼ºå°‘ GOOGLE_API_KEYï¼Œè¯·å…ˆåœ¨ Streamlit Secrets ä¸­é…ç½®åå†åˆ·æ–°ã€‚")
    st.stop()

client: Optional[OpenAI] = None
if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)

# =========================
# é¡µé¢æ ‡é¢˜
# =========================
st.title("ğŸœ Restaurant Competitor Analyzer")
st.write(
    "é¢å‘é¤å…è€æ¿çš„ä¸€é”®ä½“æ£€ï¼š\n"
    "- åªéœ€è¾“å…¥åœ°å€ï¼Œè‡ªåŠ¨åŒ¹é…ä½ çš„é¤å…\n"
    "- è‡ªåŠ¨æ‰¾é™„è¿‘ç«äº‰å¯¹æ‰‹\n"
    "- ä¼°ç®—å ‚é£Ÿ/å¤–å–çš„æ½œåœ¨æµå¤±è¥æ”¶\n"
    "- ä½¿ç”¨ ChatGPT åšèœç³»åˆ†æè¿è¥å»ºè®®"
)

# =========================
# Session State åˆå§‹åŒ–
# =========================
if "candidate_places" not in st.session_state:
    st.session_state["candidate_places"] = []
if "selected_index" not in st.session_state:
    st.session_state["selected_index"] = 0

# =========================
# å·¥å…·å‡½æ•°ï¼ˆå¸¦ç¼“å­˜ï¼‰
# =========================

@st.cache_data(show_spinner=False)
def gm_client(key: str):
    return googlemaps.Client(key=key)

@st.cache_data(show_spinner=False)
def google_geocode(api_key: str, address: str) -> List[Dict[str, Any]]:
    gmaps = gm_client(api_key)
    return gmaps.geocode(address)

@st.cache_data(show_spinner=False)
def google_place_details(api_key: str, place_id: str) -> Dict[str, Any]:
    """
    Google Place Detailsï¼š
    å…ˆå°è¯•å¸¦ fieldsï¼Œå¦‚æœ SDK/ç‰ˆæœ¬ä¸æ”¯æŒå°± fallback åˆ°ä¸å¸¦ fields çš„è°ƒç”¨ï¼Œ
    é¿å… ValueError.
    """
    gmaps = gm_client(api_key)
    fields = [
        "name",
        "formatted_address",
        "formatted_phone_number",
        "geometry",
        "rating",
        "user_ratings_total",
        "types",
        "opening_hours",
        "website",
        "price_level",
        "photos",
    ]
    try:
        result = gmaps.place(place_id=place_id, fields=fields)
        data = result.get("result", result)
    except Exception:
        # å›é€€ï¼šä¸ä¼  fieldsï¼Œæ‹¿å…¨éƒ¨å­—æ®µ
        result = gmaps.place(place_id=place_id)
        data = result.get("result", result)
    return data

@st.cache_data(show_spinner=False)
def google_places_nearby(
    api_key: str, lat: float, lng: float, radius_m: int, type_: str = "restaurant"
) -> List[Dict[str, Any]]:
    gmaps = gm_client(api_key)
    result = gmaps.places_nearby(
        location=(lat, lng), radius=radius_m, type=type_
    )
    return result.get("results", [])

@st.cache_data(show_spinner=False)
def fetch_html(url: str) -> Optional[str]:
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Restaurant-Analyzer)"}
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            return resp.text
        return None
    except Exception:
        return None

@st.cache_data(show_spinner=False)
def serpapi_google_maps_search(
    serpapi_key: str, query: str, lat: float, lng: float, zoom: float = 13.0
) -> Dict[str, Any]:
    url = "https://serpapi.com/search"
    ll_param = f"@{lat},{lng},{zoom}z"
    params = {
        "engine": "google_maps",
        "type": "search",
        "q": query,
        "ll": ll_param,
        "api_key": serpapi_key,
    }
    resp = requests.get(url, params=params, timeout=30)
    resp.raise_for_status()
    return resp.json()

# =========================
# è¯„åˆ†å‡½æ•°
# =========================

def score_gbp_profile(place: Dict[str, Any]) -> Dict[str, Any]:
    """ç®€åŒ–ç‰ˆ Google å•†å®¶èµ„æ–™è¯„åˆ†ï¼Œæ€»åˆ† 40 åˆ†ã€‚"""
    score = 0
    checks = {}

    has_name = bool(place.get("name"))
    has_address = bool(place.get("formatted_address"))
    pts = 4 if (has_name and has_address) else 0
    score += pts
    checks["åç§°/åœ°å€å®Œæ•´"] = (pts, has_name and has_address)

    has_phone = bool(place.get("formatted_phone_number"))
    pts = 4 if has_phone else 0
    score += pts
    checks["ç”µè¯"] = (pts, has_phone)

    opening_hours = place.get("opening_hours", {})
    has_hours = bool(opening_hours.get("weekday_text")) or opening_hours.get(
        "open_now"
    ) is not None
    pts = 4 if has_hours else 0
    score += pts
    checks["è¥ä¸šæ—¶é—´"] = (pts, has_hours)

    has_website = bool(place.get("website"))
    pts = 4 if has_website else 0
    score += pts
    checks["ç½‘ç«™é“¾æ¥"] = (pts, has_website)

    rating = place.get("rating")
    reviews = place.get("user_ratings_total", 0)
    has_reviews = rating is not None and reviews >= 10
    pts = 6 if has_reviews else 0
    score += pts
    checks["è¯„åˆ† & â‰¥10æ¡è¯„è®º"] = (pts, has_reviews)

    types_ = place.get("types", [])
    has_category = any(t for t in types_ if t != "point_of_interest")
    pts = 6 if has_category else 0
    score += pts
    checks["ç±»åˆ«è®¾ç½®"] = (pts, has_category)

    has_price_level = place.get("price_level") is not None
    pts = 4 if has_price_level else 0
    score += pts
    checks["ä»·æ ¼åŒºé—´"] = (pts, has_price_level)

    photos = place.get("photos", [])
    has_photos = len(photos) > 0
    pts = 8 if has_photos else 0
    score += pts
    checks["ç…§ç‰‡/å›¾ç‰‡"] = (pts, has_photos)

    return {"score": score, "checks": checks}

def score_website_basic(url: str, html: Optional[str]) -> Dict[str, Any]:
    """ç®€åŒ–ç‰ˆç½‘ç«™è¯„åˆ†ï¼Œæ€»åˆ† 40 åˆ† + è¿”å›æ–‡æœ¬æ‘˜è¦ã€‚"""
    if not url or not html:
        return {
            "score": 0,
            "checks": {"æ— æ³•è®¿é—®ç½‘ç«™": (0, False)},
            "word_count": 0,
            "title": "",
            "text_snippet": "",
        }

    soup = BeautifulSoup(html, "lxml")
    score = 0
    checks = {}

    texts = soup.get_text(separator=" ", strip=True)
    word_count = len(texts.split())
    text_snippet = texts[:3000]

    title = soup.title.string.strip() if soup.title and soup.title.string else ""
    has_title = bool(title)
    pts = 6 if has_title else 0
    score += pts
    checks["æœ‰é¡µé¢æ ‡é¢˜ï¼ˆtitleï¼‰"] = (pts, has_title)

    desc_tag = soup.find("meta", attrs={"name": "description"})
    has_desc = bool(desc_tag and desc_tag.get("content"))
    pts = 6 if has_desc else 0
    score += pts
    checks["æœ‰ Meta Description"] = (pts, has_desc)

    h1 = soup.find("h1")
    has_h1 = bool(h1 and h1.get_text(strip=True))
    pts = 4 if has_h1 else 0
    score += pts
    checks["æœ‰ H1 æ ‡é¢˜"] = (pts, has_h1)

    has_sufficient_text = word_count >= 300
    pts = 8 if has_sufficient_text else 0
    score += pts
    checks["æ–‡æœ¬é‡ â‰¥ 300 è¯"] = (pts, has_sufficient_text)

    has_phone_text = any(x in texts for x in ["(", ")", "-", "+1"])
    pts = 4 if has_phone_text else 0
    score += pts
    checks["é¡µé¢ä¸Šèƒ½çœ‹åˆ°ç”µè¯"] = (pts, has_phone_text)

    keywords = [
        "chinese", "cantonese", "szechuan", "sichuan", "shanghai",
        "dim sum", "noodle", "rice", "dumpling", "hot pot", "bbq"
    ]
    kw_hit = any(kw.lower() in texts.lower() for kw in keywords)
    pts = 6 if kw_hit else 0
    score += pts
    checks["æ–‡æœ¬åŒ…å«èœå“/èœç³»å…³é”®è¯"] = (pts, kw_hit)

    parsed = urlparse(url)
    has_https = parsed.scheme == "https"
    pts = 6 if has_https else 0
    score += pts
    checks["ä½¿ç”¨ HTTPS"] = (pts, has_https)

    return {
        "score": score,
        "checks": checks,
        "word_count": word_count,
        "title": title,
        "text_snippet": text_snippet,
    }

def estimate_revenue_loss(
    monthly_search_volume: int,
    rank_bucket: str,
    avg_order_value: float,
    channel: str = "dine-in",
) -> float:
    """ç²—ç•¥è¥æ”¶æŸå¤±ä¼°ç®—ï¼ˆå†…éƒ¨ CTR/è½¬åŒ–ç‡å‡è®¾ï¼‰ã€‚"""
    if channel == "delivery":
        ctr = 0.18
        conv = 0.35
    else:
        ctr = 0.12
        conv = 0.25

    ideal_customers = monthly_search_volume * ctr * conv
    if rank_bucket == "top3":
        current_factor = 1.0
    elif rank_bucket == "4-10":
        current_factor = 0.4
    else:
        current_factor = 0.1

    current_customers = ideal_customers * current_factor
    potential_extra_customers = ideal_customers - current_customers
    monthly_loss = potential_extra_customers * avg_order_value
    return monthly_loss

def infer_rank_from_serpapi(
    serp_json: Dict[str, Any], business_name: str
) -> Optional[int]:
    """ä» SerpAPI Google Maps ç»“æœä¸­æ‰¾åˆ°å½“å‰é¤å…åæ¬¡ã€‚"""
    results = serp_json.get("local_results") or serp_json.get("places_results") or []
    for idx, res in enumerate(results, start=1):
        name = res.get("title") or res.get("name", "")
        if name and business_name.lower() in name.lower():
            return idx
    return None

# =========================
# ChatGPT æ·±åº¦åˆ†æå‡½æ•°
# =========================

def llm_deep_analysis(
    place_detail: Dict[str, Any],
    gbp_result: Dict[str, Any],
    web_result: Dict[str, Any],
    competitors_df: Optional[pd.DataFrame],
    rank_results: List[Dict[str, Any]],
    monthly_search_volume: int,
    dine_in_aov: float,
    delivery_aov: float,
) -> str:
    if client is None:
        return "æœªé…ç½® OPENAI_API_KEYï¼Œæ— æ³•è°ƒç”¨ ChatGPTã€‚"

    comp_json = []
    if competitors_df is not None and not competitors_df.empty:
        sub = competitors_df.head(5)
        comp_json = sub.to_dict(orient="records")

    payload = {
        "restaurant": {
            "name": place_detail.get("name"),
            "address": place_detail.get("formatted_address"),
            "phone": place_detail.get("formatted_phone_number"),
            "types": place_detail.get("types", []),
            "rating": place_detail.get("rating"),
            "reviews": place_detail.get("user_ratings_total"),
            "price_level": place_detail.get("price_level"),
        },
        "gbp_score": gbp_result["score"],
        "gbp_checks": gbp_result["checks"],
        "website_score": web_result["score"],
        "website_title": web_result.get("title", ""),
        "website_word_count": web_result.get("word_count", 0),
        "competitors": comp_json,
        "rank_results": rank_results,
        "assumptions": {
            "monthly_search_volume_per_keyword": monthly_search_volume,
            "dine_in_aov": dine_in_aov,
            "delivery_aov": delivery_aov,
        },
    }

    text_snippet = web_result.get("text_snippet", "")

    system_msg = (
         "ä½ æ˜¯ä¸€åä¸“é—¨æœåŠ¡åŒ—ç¾é¤é¦†çš„æœ¬åœ°è¥é”€å’Œå¤–å–è¿è¥é¡¾é—®ï¼Œæ›¾ä»»èŒäºéº¦è‚¯é”¡ä¸€ä¸ªä¸“é—¨åšé¤é¥®åˆ†æçš„éƒ¨é—¨"
         "éå¸¸äº†è§£ä¸–ç•Œå„åœ°çš„èœç³»ï¼Œå°¤å…¶åœ¨ä¸­é¤èœç³»çš„ç»†åˆ†é¢†åŸŸå±äºè¡Œä¸šæƒå¨ï¼Œå¦‚ç²¤èœã€èŒ¶é¤å…ã€å·èœã€æ¹˜èœã€ä¸œåŒ—èœã€ä¸Šæµ·èœç­‰ç»†åˆ†èœç³»ï¼Œ"
         "ç†Ÿæ‚‰ Google æœ¬åœ°æœç´¢å’Œ UberEats/DoorDash/Grubhub/Hungrypanda/Fantuan ç­‰å¹³å°çš„è¿è¥é€»è¾‘ã€‚"
         "è¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ï¼Œä½†åœ¨éœ€è¦æ—¶å¯åŠ å°‘é‡è‹±æ–‡æœ¯è¯­ã€‚"
    )

    user_msg = f"""
è¿™æ˜¯ä¸€ä¸ªé¤å…çš„åœ¨çº¿æ•°æ®ï¼Œè¯·ä½ åš**å¤šç»´æ·±åº¦åˆ†æ**å¹¶ç»™å‡ºç»†åˆ†èœç³»åˆ¤æ–­ä¸è¿è¥å»ºè®®ã€‚

ã€ç»“æ„åŒ–æ•°æ® JSONã€‘
{json.dumps(payload, ensure_ascii=False, indent=2)}

ã€ç½‘ç«™æ–‡æœ¬ç‰‡æ®µï¼ˆæœ€å¤š 3000 å­—ç¬¦ï¼‰ã€‘
{text_snippet}

è¯·ä½ å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. èœç³»ç»†åˆ†åˆ¤æ–­â€¦â€¦
ï¼ˆåé¢åŒä¹‹å‰ç‰ˆæœ¬ï¼Œç•¥ï¼‰
"""

    completion = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.4,
    )

    return completion.choices[0].message.content

# =========================
# ä¸»ç•Œé¢ï¼š1 åœ°å€ â†’ å€™é€‰é¤å…
# =========================

st.markdown("## 1ï¸âƒ£ è¾“å…¥é¤å…åœ°å€ï¼ˆè‡ªåŠ¨åŒ¹é…é™„è¿‘é¤å…ï¼‰")

address_input = st.text_input(
    "é¤å…åœ°å€ï¼ˆä¾‹å¦‚ï¼š1115 Clement St, San Francisco, CAï¼‰",
    "",
    help="å¯ä»¥æ˜¯å®Œæ•´åœ°å€æˆ–è¡—é“ + åŸå¸‚ï¼Œç³»ç»Ÿä¼šç”¨ Google è‡ªåŠ¨åŒ¹é…é™„è¿‘çš„é¤å…ã€‚",
)

search_btn = st.button("ğŸ” æ ¹æ®åœ°å€æŸ¥æ‰¾é™„è¿‘é¤å…")

if search_btn:
    if not address_input.strip():
        st.error("è¯·å…ˆè¾“å…¥åœ°å€ã€‚")
    else:
        with st.spinner("æ ¹æ®åœ°å€å®šä½å¹¶æŸ¥æ‰¾é™„è¿‘é¤å…..."):
            geocode_res = google_geocode(GOOGLE_API_KEY, address_input)
            if not geocode_res:
                st.error("æ— æ³•é€šè¿‡è¯¥åœ°å€æ‰¾åˆ°ä½ç½®ï¼Œè¯·æ£€æŸ¥æ‹¼å†™ã€‚")
            else:
                loc = geocode_res[0]["geometry"]["location"]
                lat = loc["lat"]
                lng = loc["lng"]
                nearby = google_places_nearby(
                    GOOGLE_API_KEY, lat, lng, radius_m=300, type_="restaurant"
                )
                if not nearby:
                    st.warning("é™„è¿‘ 300 ç±³å†…æœªæ‰¾åˆ°é¤å…ï¼Œè¯·å°è¯•è¾“å…¥æ›´ç²¾ç¡®çš„åœ°å€æˆ–æ”¾å¤§èŒƒå›´ã€‚")
                else:
                    st.session_state["candidate_places"] = nearby
                    st.success(f"å·²æ‰¾åˆ° {len(nearby)} å®¶é™„è¿‘é¤å…ï¼Œè¯·åœ¨ä¸‹æ–¹é€‰æ‹©ä½ çš„é¤å…ã€‚")

# =========================
# 2 é€‰æ‹©é¤å… + ä¸šåŠ¡å‚æ•°
# =========================

candidate_places = st.session_state["candidate_places"]
selected_place_id: Optional[str] = None
place_label_list: List[str] = []

if candidate_places:
    st.markdown("### é€‰æ‹©ä½ çš„é¤å…")

    for p in candidate_places:
        label = f"{p.get('name', 'Unnamed')} â€” {p.get('vicinity', '')}"
        place_label_list.append(label)

    selected_index = st.selectbox(
        "åœ¨é™„è¿‘é¤å…åˆ—è¡¨ä¸­é€‰æ‹©ä½ è¦åˆ†æçš„é‚£ä¸€å®¶ï¼š",
        options=list(range(len(place_label_list))),
        format_func=lambda i: place_label_list[i],
        index=st.session_state.get("selected_index", 0),
    )
    st.session_state["selected_index"] = selected_index
    selected_place_id = candidate_places[selected_index]["place_id"]

    st.markdown("### å¡«å†™ä¸šåŠ¡å‚æ•°")

    col_aov1, col_aov2 = st.columns(2)
    with col_aov1:
        dine_in_aov = st.number_input(
            "å ‚é£Ÿå¹³å‡å®¢å•ä»·ï¼ˆUSDï¼‰",
            min_value=5.0,
            max_value=300.0,
            value=35.0,
            step=1.0,
        )
    with col_aov2:
        delivery_aov = st.number_input(
            "å¤–å–å¹³å‡å®¢å•ä»·ï¼ˆUSDï¼‰",
            min_value=5.0,
            max_value=300.0,
            value=45.0,
            step=1.0,
        )

    st.markdown("### å…³é”®è¯ & æœç´¢é‡ï¼ˆä¸æ‡‚å°±ç”¨é»˜è®¤å€¼ï¼‰")

    keywords_input = st.text_input(
        "æ ¸å¿ƒå…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰",
        "best chinese food, best asian food, best baked chicken",
        help="ç”¨äºä¼°ç®—ä½ åœ¨ Google æœ¬åœ°æœç´¢é‡Œçš„æœºä¼šã€‚ä¸æ‡‚å°±ç”¨é»˜è®¤å€¼ã€‚",
    )

    monthly_search_volume = st.number_input(
        "ä¼°ç®—æ¯ä¸ªæ ¸å¿ƒå…³é”®è¯çš„æœˆæœç´¢é‡ï¼ˆç»Ÿä¸€ç²—ç•¥å€¼ï¼‰",
        min_value=50,
        max_value=50000,
        value=500,
        step=50,
        help="ç®€å•ç†è§£ä¸ºï¼šè¿™ä¸€ç±»å…³é”®è¯å¤§æ¦‚æ¯æœˆæœ‰å¤šå°‘äººæœç´¢ã€‚",
    )

    website_override = st.text_input(
        "å¦‚æœä½ çš„å®˜ç½‘å’Œ Google é‡Œè®°å½•çš„ä¸ä¸€æ ·ï¼Œåœ¨è¿™é‡Œå¡«ä½ çš„å®˜ç½‘ URLï¼ˆå¯é€‰ï¼‰",
        "",
    )

    run_btn = st.button("ğŸš€ è¿è¡Œåˆ†æ")

else:
    st.info("å…ˆè¾“å…¥åœ°å€å¹¶ç‚¹å‡»â€œæ ¹æ®åœ°å€æŸ¥æ‰¾é™„è¿‘é¤å…â€ã€‚")

# =========================
# 3 ä¸»åˆ†æé€»è¾‘
# =========================

if candidate_places and selected_place_id and "run_btn" in locals() and run_btn:
    with st.spinner("è·å–é¤å…è¯¦æƒ…ï¼ˆGoogle Place Detailsï¼‰..."):
        place_detail = google_place_details(GOOGLE_API_KEY, selected_place_id)

    st.success(f"å·²é”å®šé¤å…ï¼š**{place_detail.get('name', 'Unknown')}**")

    # ä¸‹é¢é€»è¾‘ä¸ä¹‹å‰ä¸€è‡´ï¼šç«äº‰å¯¹æ‰‹ã€GBP è¯„åˆ†ã€ç½‘ç«™è¯„åˆ†ã€å…³é”®è¯æ’åã€AI åˆ†æâ€¦â€¦
    #ï¼ˆä¸ºäº†ä¸è¶…å­—æ•°ï¼Œå°±ä¸å†å…¨éƒ¨é‡å¤å±•å¼€ï¼Œå¦‚æœä½ éœ€è¦æˆ‘ä¹Ÿå¯ä»¥å†ç»™ä½ ä¸€ä»½å®Œæ•´å±•å¼€ç‰ˆï¼‰
# ========== ç½²åï¼ˆLinkedInï¼‰ ==========
LINKEDIN_URL = "https://www.linkedin.com/in/lingyu-maxwell-lai"
st.markdown(
    f"""
<div style="display:flex;align-items:center;gap:10px;margin-top:-6px;margin-bottom:8px;">
  <div style="font-size:14px;color:#666;">
    Builded by <strong>Maxwell Lai</strong>
  </div>
  <a href="{LINKEDIN_URL}" target="_blank" title="LinkedIn: Maxwell Lai"
     style="display:inline-flex;align-items:center;justify-content:center;width:18px;height:18px;
            border-radius:4px;background:#0A66C2;">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/linkedin.svg"
         alt="LinkedIn" width="12" height="12" style="filter: invert(1);" />
  </a>
</div>
""",
    unsafe_allow_html=True,
)
