import json
from typing import List, Dict, Any, Optional

import streamlit as st
import pandas as pd
import requests
import googlemaps
from bs4 import BeautifulSoup
from urllib.parse import urlparse

from openai import OpenAI

# =========================
# åŸºæœ¬é…ç½® & Secrets
# =========================
st.set_page_config(
    page_title="Aurainsight é¤é¦†å¢é•¿è¯Šæ–­",
    layout="wide",
)

GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY", "")
SERPAPI_KEY = st.secrets.get("SERPAPI_KEY", "")
YELP_API_KEY = st.secrets.get("YELP_API_KEY", "")
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")
HEADLESS_ENABLED_RAW = st.secrets.get("HEADLESS_ENABLED", "false")

HEADLESS_ENABLED = str(HEADLESS_ENABLED_RAW).strip().lower() in ["1", "true", "yes"]

if not GOOGLE_API_KEY:
    st.error("ç¼ºå°‘ GOOGLE_API_KEYï¼Œè¯·å…ˆåœ¨ Streamlit Secrets ä¸­é…ç½®åå†åˆ·æ–°ã€‚")
    st.stop()

client: Optional[OpenAI] = None
if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)

# =========================
# é¡µé¢æ ‡é¢˜
# =========================
st.title("Aurainsight é¤é¦†å¢é•¿è¯Šæ–­")

st.write(
    "é¢å‘é¤å…è€æ¿çš„ä¸€é”®ä½“æ£€ï¼š\n"
    "- åªéœ€è¾“å…¥åœ°å€ï¼Œè‡ªåŠ¨åŒ¹é…ä½ çš„é¤å…\n"
    "- è‡ªåŠ¨æ‰¾é™„è¿‘ç«äº‰å¯¹æ‰‹\n"
    "- ä¼°ç®—å ‚é£Ÿ/å¤–å–çš„æ½œåœ¨æµå¤±è¥æ”¶\n"
    "- å¯é€‰ï¼šè´´ä¸Šå¤–å– / åœ¨çº¿ç‚¹é¤é“¾æ¥ï¼Œåšèœå•çº§åˆ«åˆ†æ\n"
    "- ä½¿ç”¨ ChatGPT è¾“å‡ºç»†åˆ†èœç³» & èœå•ç»“æ„ & è¿è¥å»ºè®®"
)

# =========================
# Session State åˆå§‹åŒ–
# =========================
if "candidate_places" not in st.session_state:
    st.session_state["candidate_places"] = []
if "selected_index" not in st.session_state:
    st.session_state["selected_index"] = 0
if "run_analysis" not in st.session_state:
    st.session_state["run_analysis"] = False

# =========================
# Google Maps ç›¸å…³å·¥å…·å‡½æ•°ï¼ˆå¸¦ç¼“å­˜ï¼‰
# =========================

@st.cache_data(show_spinner=False)
def gm_client(key: str):
    return googlemaps.Client(key=key)

@st.cache_data(show_spinner=False)
def google_geocode(api_key: str, address: str) -> List[Dict[str, Any]]:
    gmaps = gm_client(api_key)
    return gmaps.geocode(address)

@st.cache_data(show_spinner=False)
def google_place_details(api_key: str, place_id: str) -> Dict[str, Any]:
    """
    Google Place Detailsï¼š
    å…ˆå°è¯•å¸¦ fieldsï¼Œå¦‚æœ SDK/ç‰ˆæœ¬ä¸æ”¯æŒå°± fallback åˆ°ä¸å¸¦ fields çš„è°ƒç”¨ã€‚
    """
    gmaps = gm_client(api_key)
    fields = [
        "name",
        "formatted_address",
        "formatted_phone_number",
        "geometry",
        "rating",
        "user_ratings_total",
        "types",
        "opening_hours",
        "website",
        "price_level",
        "photos",
    ]
    try:
        result = gmaps.place(place_id=place_id, fields=fields)
        data = result.get("result", result)
    except Exception:
        result = gmaps.place(place_id=place_id)
        data = result.get("result", result)
    return data

@st.cache_data(show_spinner=False)
def google_places_nearby(
    api_key: str, lat: float, lng: float, radius_m: int, type_: str = "restaurant"
) -> List[Dict[str, Any]]:
    gmaps = gm_client(api_key)
    result = gmaps.places_nearby(
        location=(lat, lng), radius=radius_m, type=type_
    )
    return result.get("results", [])

@st.cache_data(show_spinner=False)
def serpapi_google_maps_search(
    serpapi_key: str, query: str, lat: float, lng: float, zoom: float = 13.0
) -> Dict[str, Any]:
    url = "https://serpapi.com/search"
    ll_param = f"@{lat},{lng},{zoom}z"
    params = {
        "engine": "google_maps",
        "type": "search",
        "q": query,
        "ll": ll_param,
        "api_key": serpapi_key,
    }
    resp = requests.get(url, params=params, timeout=30)
    resp.raise_for_status()
    return resp.json()

# =========================
# èœå•æŠ“å–ï¼šrequests + å¯é€‰ headless æµè§ˆå™¨
# =========================

@st.cache_data(show_spinner=False)
def fetch_html_requests(url: str) -> Optional[str]:
    """å¸¸è§„ requests æŠ“å–ï¼Œå®½æ¾æ¥å— 3xxã€‚"""
    try:
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0 Safari/537.36"
            ),
            "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
        }
        resp = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
        if resp.status_code < 400:
            return resp.text
        st.warning(f"[èœå•æŠ“å–] {url} è¿”å›çŠ¶æ€ç  {resp.status_code}ã€‚")
        return None
    except Exception as e:
        st.warning(f"[èœå•æŠ“å–] è®¿é—® {url} å‡ºé”™ï¼š{e}")
        return None

@st.cache_data(show_spinner=False)
def fetch_html_headless(url: str) -> Optional[str]:
    """
    ä½¿ç”¨ Playwright çš„ headless Chromium æŠ“é¡µé¢å†…å®¹ã€‚
    éœ€è¦åœ¨è¿è¡Œç¯å¢ƒä¸­å®‰è£…ï¼š
      pip install playwright
      playwright install chromium
    åœ¨ Streamlit Cloud ä¸Šå¯èƒ½æ— æ³•ä½¿ç”¨ï¼Œä»…é€‚åˆæœ¬åœ° / è‡ªå»ºæœåŠ¡å™¨ã€‚
    """
    if not HEADLESS_ENABLED:
        return None

    try:
        from playwright.sync_api import sync_playwright
    except ImportError:
        st.warning("æœªå®‰è£… playwrightï¼Œæ— æ³•ä½¿ç”¨æ— å¤´æµè§ˆå™¨æŠ“å–ã€‚")
        return None

    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/120.0 Safari/537.36"
                ),
                locale="en-US",
            )
            page = context.new_page()
            page.goto(url, wait_until="networkidle", timeout=25000)
            content = page.content()
            browser.close()
            return content
    except Exception as e:
        st.warning(f"[Headless æŠ“å–] è®¿é—® {url} å‡ºé”™ï¼š{e}")
        return None

@st.cache_data(show_spinner=False)
def fetch_html(url: str) -> Optional[str]:
    """
    ç»Ÿä¸€å…¥å£ï¼š
    1) å…ˆå°è¯• requests
    2) ä¸è¡Œå†å°è¯• headlessï¼ˆå¦‚æœå¯ç”¨ä¸”å¯ç”¨ï¼‰
    """
    html = fetch_html_requests(url)
    if html:
        return html

    # requests æŠ“ä¸åˆ°ï¼Œå†è¯• headless
    html2 = fetch_html_headless(url)
    return html2

def clean_text_block(text: str) -> str:
    """ç®€å•æ¸…æ´—æ–‡æœ¬ï¼Œå»æ‰å¤šä½™ç©ºç™½ã€‚"""
    lines = [ln.strip() for ln in text.splitlines()]
    lines = [ln for ln in lines if ln]
    return "\n".join(lines)

def extract_menu_text_from_html(html: str) -> str:
    """
    é€šç”¨ç‰ˆèœå•æŠ“å–ï¼š
    - ç§»é™¤ script/style
    - ä¼˜å…ˆå¯»æ‰¾åŒ…å«ä»·æ ¼ç¬¦å·/å¸¸è§èœå“å…³é”®è¯çš„è¡Œ
    - ä¸è¿½æ±‚ 100% ç»“æ„åŒ–ï¼Œåªè¦ç»™ LLM è¶³å¤Ÿçš„åŠç»“æ„åŒ–ä¿¡æ¯
    """
    soup = BeautifulSoup(html, "lxml")

    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    raw_text = soup.get_text("\n", strip=True)
    raw_text = clean_text_block(raw_text)

    lines = raw_text.split("\n")
    menu_lines = []

    price_tokens = ["$", "ï¿¥", "Â£", "â‚¬"]
    food_keywords = [
        "chicken", "beef", "pork", "tofu", "noodle", "rice", "dumpling",
        "soup", "fried", "braised", "spicy", "bbq", "curry",
        "é¥­", "é¢", "ç²‰", "é¸¡", "ç‰›", "çŒª", "æ±¤", "ç‚’", "ç…", "ç„—", "å’–å–±",
    ]

    for ln in lines:
        l = ln.lower()
        if any(p in ln for p in price_tokens) or any(k in l for k in food_keywords):
            menu_lines.append(ln)

    # å¦‚æœæŠ“ä¸åˆ°æ˜æ˜¾èœå•çº¿ï¼Œå°±é€€å›è¾ƒé•¿æ–‡æœ¬çš„ä¸€éƒ¨åˆ†
    if len(menu_lines) < 10:
        menu_lines = lines

    menu_text = "\n".join(menu_lines)
    return menu_text[:6000]

def build_menu_payload(menu_urls: List[str]) -> List[Dict[str, str]]:
    """
    æ ¹æ®ç”¨æˆ·è¾“å…¥çš„èœå• URL åˆ—è¡¨ï¼ŒæŠ“å–æ–‡æœ¬å¹¶æ„é€  LLM ä½¿ç”¨çš„ç»“æ„ã€‚
    """
    menus: List[Dict[str, str]] = []
    for url in menu_urls:
        url = url.strip()
        if not url:
            continue

        html = fetch_html(url)
        if not html:
            menus.append(
                {
                    "source": urlparse(url).netloc or "unknown",
                    "url": url,
                    "status": "fetch_failed_or_blocked",
                    "menu_text": "",
                }
            )
            continue

        menu_text = extract_menu_text_from_html(html)
        status = "ok" if menu_text.strip() else "no_menu_detected"

        menus.append(
            {
                "source": urlparse(url).netloc or "unknown",
                "url": url,
                "status": status,
                "menu_text": menu_text,
            }
        )

    return menus

# =========================
# è¯„åˆ†å‡½æ•°
# =========================

def score_gbp_profile(place: Dict[str, Any]) -> Dict[str, Any]:
    """ç®€åŒ–ç‰ˆ Google å•†å®¶èµ„æ–™è¯„åˆ†ï¼Œæ€»åˆ† 40 åˆ†ã€‚"""
    score = 0
    checks = {}

    has_name = bool(place.get("name"))
    has_address = bool(place.get("formatted_address"))
    pts = 4 if (has_name and has_address) else 0
    score += pts
    checks["åç§°/åœ°å€å®Œæ•´"] = (pts, has_name and has_address)

    has_phone = bool(place.get("formatted_phone_number"))
    pts = 4 if has_phone else 0
    score += pts
    checks["ç”µè¯"] = (pts, has_phone)

    opening_hours = place.get("opening_hours", {})
    has_hours = bool(opening_hours.get("weekday_text")) or opening_hours.get(
        "open_now"
    ) is not None
    pts = 4 if has_hours else 0
    score += pts
    checks["è¥ä¸šæ—¶é—´"] = (pts, has_hours)

    has_website = bool(place.get("website"))
    pts = 4 if has_website else 0
    score += pts
    checks["ç½‘ç«™é“¾æ¥"] = (pts, has_website)

    rating = place.get("rating")
    reviews = place.get("user_ratings_total", 0)
    has_reviews = rating is not None and reviews >= 10
    pts = 6 if has_reviews else 0
    score += pts
    checks["è¯„åˆ† & â‰¥10æ¡è¯„è®º"] = (pts, has_reviews)

    types_ = place.get("types", [])
    has_category = any(t for t in types_ if t != "point_of_interest")
    pts = 6 if has_category else 0
    score += pts
    checks["ç±»åˆ«è®¾ç½®"] = (pts, has_category)

    has_price_level = place.get("price_level") is not None
    pts = 4 if has_price_level else 0
    score += pts
    checks["ä»·æ ¼åŒºé—´"] = (pts, has_price_level)

    photos = place.get("photos", [])
    has_photos = len(photos) > 0
    pts = 8 if has_photos else 0
    score += pts
    checks["ç…§ç‰‡/å›¾ç‰‡"] = (pts, has_photos)

    return {"score": score, "checks": checks}

def score_website_basic(url: str, html: Optional[str]) -> Dict[str, Any]:
    """ç®€åŒ–ç‰ˆç½‘ç«™è¯„åˆ†ï¼Œæ€»åˆ† 40 åˆ† + è¿”å›æ–‡æœ¬æ‘˜è¦ã€‚"""
    if not url or not html:
        return {
            "score": 0,
            "checks": {"æ— æ³•è®¿é—®ç½‘ç«™": (0, False)},
            "word_count": 0,
            "title": "",
            "text_snippet": "",
        }

    soup = BeautifulSoup(html, "lxml")
    score = 0
    checks = {}

    texts = soup.get_text(separator=" ", strip=True)
    word_count = len(texts.split())
    text_snippet = texts[:3000]

    title = soup.title.string.strip() if soup.title and soup.title.string else ""
    has_title = bool(title)
    pts = 6 if has_title else 0
    score += pts
    checks["æœ‰é¡µé¢æ ‡é¢˜ï¼ˆtitleï¼‰"] = (pts, has_title)

    desc_tag = soup.find("meta", attrs={"name": "description"})
    has_desc = bool(desc_tag and desc_tag.get("content"))
    pts = 6 if has_desc else 0
    score += pts
    checks["æœ‰ Meta Description"] = (pts, has_desc)

    h1 = soup.find("h1")
    has_h1 = bool(h1 and h1.get_text(strip=True))
    pts = 4 if has_h1 else 0
    score += pts
    checks["æœ‰ H1 æ ‡é¢˜"] = (pts, has_h1)

    has_sufficient_text = word_count >= 300
    pts = 8 if has_sufficient_text else 0
    score += pts
    checks["æ–‡æœ¬é‡ â‰¥ 300 è¯"] = (pts, has_sufficient_text)

    has_phone_text = any(x in texts for x in ["(", ")", "-", "+1"])
    pts = 4 if has_phone_text else 0
    score += pts
    checks["é¡µé¢ä¸Šèƒ½çœ‹åˆ°ç”µè¯"] = (pts, has_phone_text)

    keywords = [
        "chinese", "cantonese", "szechuan", "sichuan", "shanghai",
        "dim sum", "noodle", "rice", "dumpling", "hot pot", "bbq"
    ]
    kw_hit = any(kw.lower() in texts.lower() for kw in keywords)
    pts = 6 if kw_hit else 0
    score += pts
    checks["æ–‡æœ¬åŒ…å«èœå“/èœç³»å…³é”®è¯"] = (pts, kw_hit)

    parsed = urlparse(url)
    has_https = parsed.scheme == "https"
    pts = 6 if has_https else 0
    score += pts
    checks["ä½¿ç”¨ HTTPS"] = (pts, has_https)

    return {
        "score": score,
        "checks": checks,
        "word_count": word_count,
        "title": title,
        "text_snippet": text_snippet,
    }

def estimate_revenue_loss(
    monthly_search_volume: int,
    rank_bucket: str,
    avg_order_value: float,
    channel: str = "dine-in",
) -> float:
    """ç²—ç•¥è¥æ”¶æŸå¤±ä¼°ç®—ï¼ˆå†…éƒ¨ CTR/è½¬åŒ–ç‡å‡è®¾ï¼‰ã€‚"""
    if channel == "delivery":
        ctr = 0.18
        conv = 0.35
    else:
        ctr = 0.12
        conv = 0.25

    ideal_customers = monthly_search_volume * ctr * conv
    if rank_bucket == "top3":
        current_factor = 1.0
    elif rank_bucket == "4-10":
        current_factor = 0.4
    else:
        current_factor = 0.1

    current_customers = ideal_customers * current_factor
    potential_extra_customers = ideal_customers - current_customers
    monthly_loss = potential_extra_customers * avg_order_value
    return monthly_loss

def infer_rank_from_serpapi(
    serp_json: Dict[str, Any], business_name: str
) -> Optional[int]:
    """ä» SerpAPI Google Maps ç»“æœä¸­æ‰¾åˆ°å½“å‰é¤å…åæ¬¡ã€‚"""
    results = serp_json.get("local_results") or serp_json.get("places_results") or []
    for idx, res in enumerate(results, start=1):
        name = res.get("title") or res.get("name", "")
        if name and business_name.lower() in name.lower():
            return idx
    return None

# =========================
# ChatGPT å°è£… & æ·±åº¦åˆ†æ
# =========================

def call_llm_safe(messages: list) -> str:
    """
    ä¼˜å…ˆå°è¯• gpt-4.1-miniï¼Œ403/æ¨¡å‹æ— æƒé™æ—¶è‡ªåŠ¨é€€å› gpt-4o-miniã€‚
    """
    if client is None:
        return "æœªé…ç½® OPENAI_API_KEYï¼Œæ— æ³•è°ƒç”¨ ChatGPTã€‚"

    primary_model = "gpt-4.1-mini"
    fallback_model = "gpt-4o-mini"

    try:
        resp = client.chat.completions.create(
            model=primary_model,
            messages=messages,
            temperature=0.4,
        )
        return resp.choices[0].message.content
    except Exception as e:
        st.warning(f"ä½¿ç”¨ {primary_model} å¤±è´¥ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° {fallback_model}ã€‚é”™è¯¯ï¼š{e}")

    try:
        resp = client.chat.completions.create(
            model=fallback_model,
            messages=messages,
            temperature=0.4,
        )
        return resp.choices[0].message.content
    except Exception as e:
        return f"è°ƒç”¨ ChatGPT å¤±è´¥ï¼š{e}"

def llm_deep_analysis(
    place_detail: Dict[str, Any],
    gbp_result: Dict[str, Any],
    web_result: Dict[str, Any],
    competitors_df: Optional[pd.DataFrame],
    rank_results: List[Dict[str, Any]],
    monthly_search_volume: int,
    dine_in_aov: float,
    delivery_aov: float,
    menus: List[Dict[str, str]],
) -> str:
    comp_json = []
    if competitors_df is not None and not competitors_df.empty:
        sub = competitors_df.head(5)
        comp_json = sub.to_dict(orient="records")

    menus_safe = []
    for m in menus:
        menus_safe.append(
            {
                "source": m.get("source", ""),
                "url": m.get("url", ""),
                "status": m.get("status", ""),
                "menu_text": (m.get("menu_text") or "")[:4000],
            }
        )

    payload = {
        "restaurant": {
            "name": place_detail.get("name"),
            "address": place_detail.get("formatted_address"),
            "phone": place_detail.get("formatted_phone_number"),
            "types": place_detail.get("types", []),
            "rating": place_detail.get("rating"),
            "reviews": place_detail.get("user_ratings_total"),
            "price_level": place_detail.get("price_level"),
        },
        "gbp_score": gbp_result["score"],
        "gbp_checks": gbp_result["checks"],
        "website_score": web_result["score"],
        "website_title": web_result.get("title", ""),
        "website_word_count": web_result.get("word_count", 0),
        "competitors": comp_json,
        "rank_results": rank_results,
        "assumptions": {
            "monthly_search_volume_per_keyword": monthly_search_volume,
            "dine_in_aov": dine_in_aov,
            "delivery_aov": delivery_aov,
        },
        "menus": menus_safe,
    }

    text_snippet = web_result.get("text_snippet", "")

    system_msg = (
        "ä½ æ˜¯ä¸€åä¸“é—¨æœåŠ¡åŒ—ç¾é¤é¦†çš„æœ¬åœ°è¥é”€å’Œå¤–å–è¿è¥é¡¾é—®ï¼Œæ›¾ä»»èŒäºéº¦è‚¯é”¡é¤é¥®é¡¹ç›®ç»„ï¼Œ"
        "éå¸¸äº†è§£ä¸–ç•Œå„åœ°çš„èœç³»ï¼Œå°¤å…¶åœ¨ä¸­é¤ç»†åˆ†é¢†åŸŸï¼ˆç²¤èœã€èŒ¶é¤å…ã€å·èœã€æ¹˜èœã€ä¸œåŒ—èœã€ä¸Šæµ·èœç­‰ï¼‰ï¼Œ"
        "ç†Ÿæ‚‰ Google æœ¬åœ°æœç´¢å’Œ UberEats / DoorDash / Grubhub / Hungrypanda / Fantuan ç­‰å¹³å°çš„è¿è¥é€»è¾‘ã€‚"
        "è¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ï¼Œä½†åœ¨éœ€è¦æ—¶å¯åŠ å°‘é‡è‹±æ–‡æœ¯è¯­ã€‚"
    )

    user_msg = f"""
ä»¥ä¸‹æ˜¯æŸå®¶é¤å…çš„çº¿ä¸Šæ•°æ®å’Œèœå•æ•°æ®ï¼Œè¯·ä½ ç»“åˆä¸€èµ·åš**å¤šç»´æ·±åº¦åˆ†æ**ï¼Œé‡ç‚¹åŒ…æ‹¬â€œèœç³»ç»†åˆ† + èœå•ç»“æ„ + æœ¬åœ°ç«äº‰ + çº¿ä¸Šè¿è¥å»ºè®®â€ã€‚

ã€ç»“æ„åŒ–æ•°æ® JSONã€‘
{json.dumps(payload, ensure_ascii=False, indent=2)}

ã€å®˜ç½‘æ–‡æœ¬ç‰‡æ®µï¼ˆæœ€å¤š 3000 å­—ç¬¦ï¼‰ã€‘
{text_snippet}

è¯·æŒ‰ä¸‹é¢ç»“æ„è¾“å‡ºï¼ˆåˆ†æˆæ¸…æ™°å°æ ‡é¢˜ï¼Œæ–¹ä¾¿é¤å…è€æ¿ç›´æ¥ç…§åšï¼‰ï¼š

1ï¸âƒ£ èœç³»ç»†åˆ†åˆ¤æ–­
- æ ¹æ®é¤å…åç§°ã€åœ°å€ã€Google ç±»å‹ã€è¯„ä»·ä»¥åŠèœå•å†…å®¹ï¼Œåˆ¤æ–­è¯¥åº—æœ€æ¥è¿‘å“ªä¸€ç±»ç»†åˆ†èœç³»ï¼š
  - ä¾‹å¦‚ï¼šæ­£å®—å·èœé¦†ã€å®¶å¸¸å·èœ + ç¾å¼ä¸­é¤æ··æ­ã€æ¸¯å¼èŒ¶é¤å…ã€ç²¤èœçƒ§è…Šã€ä¸œåŒ—èœã€ä¸Šæµ·æœ¬å¸®èœã€ç¾å¼ä¸­é¤å¤–å–åº—ç­‰ã€‚
- å†™å‡ºä½ çš„åˆ¤æ–­ä¾æ®ï¼ˆä¾‹å¦‚ï¼šèœå“åå­—ã€å£å‘³å…³é”®è¯ã€ä»·ä½åŒºé—´ã€èœå•ç»“æ„ç­‰ï¼‰ã€‚

2ï¸âƒ£ æœ¬åœ°ç«äº‰æ ¼å±€ï¼ˆåŸºäºé™„è¿‘ç«äº‰å¯¹æ‰‹ï¼‰
- å°†é™„è¿‘ä¸»è¦ç«äº‰å¯¹æ‰‹æŒ‰å¤§è‡´èœç³»åˆ†ç»„ï¼ˆå·èœã€å·æ¹˜ã€éŸ©å¼ç‚¸é¸¡ã€æ³°å›½èœã€ç¾å¼ç‚¸é¸¡ã€ä¼‘é—²ä¸­é¤ç­‰ï¼‰ã€‚
- ç®€è¦å¯¹æ¯”æœ¬åº—åœ¨ä»¥ä¸‹ç»´åº¦çš„ä¼˜åŠ£åŠ¿ï¼š
  - è¯„åˆ† & è¯„è®ºæ•°
  - ä»·æ ¼å¸¦ï¼ˆä¾¿å®œ / ä¸­ç­‰ / åè´µï¼‰
  - è®°å¿†ç‚¹ï¼ˆæ˜¯å¦æœ‰å¼ºè®°å¿†çš„æ‹›ç‰Œèœæˆ–èœç³»æ ‡ç­¾ï¼‰

3ï¸âƒ£ Google å•†å®¶èµ„æ–™ï¼ˆGBPï¼‰ä¼˜åŒ–ä¼˜å…ˆçº§
- ç»“åˆ gbp_score å’Œå„æ£€æŸ¥é¡¹ï¼Œåˆ—å‡º**æœ€ä¼˜å…ˆè¦è¡¥çš„ 3â€“5 é¡¹**ï¼Œä¾‹å¦‚ï¼š
  - ç…§ç‰‡æ•°é‡ & è´¨é‡ï¼ˆé—¨è„¸å›¾ã€èœå“å›¾ã€èœå•å›¾ï¼‰
  - è¥ä¸šæ—¶é—´ã€æœåŠ¡é€‰é¡¹ï¼ˆdine-in / takeout / deliveryï¼‰
  - å®˜ç½‘é“¾æ¥ã€é¢„è®¢/ç‚¹é¤æŒ‰é’®ç­‰
- æ¯ä¸€é¡¹éƒ½å†™æ¸…æ¥šï¼š
  - å…·ä½“è¦åšä»€ä¹ˆ
  - è¿™ä»¶äº‹ä¼šå¦‚ä½•å½±å“æ›å…‰ã€ç‚¹å‡»ç‡æˆ–ä¸‹å•ç‡

4ï¸âƒ£ å®˜ç½‘å†…å®¹ä¸è½¬åŒ–å»ºè®®
- è¯„ä»·å®˜ç½‘ç›®å‰åœ¨ä»¥ä¸‹æ–¹é¢çš„è¡¨ç°ï¼š
  - æ˜¯å¦ä¸€çœ¼çœ‹å¾—å‡ºèœç³»å’Œæ‹›ç‰Œèœ
  - æ–‡æ¡ˆæ˜¯å¦åŒºåˆ†å ‚é£Ÿ vs å¤–å–åœºæ™¯
  - æ˜¯å¦æœ‰æ¸…æ™°çš„åœ¨çº¿ç‚¹é¤ / è®¢ä½ CTAï¼ˆcall-to-actionï¼‰
- ç»™å‡º 3â€“5 æ¡éå¸¸å…·ä½“ã€èƒ½ç›´æ¥ç…§æŠ„çš„ä¼˜åŒ–å»ºè®®ï¼ˆç”¨ bullet pointsï¼‰ã€‚

5ï¸âƒ£ èœå•ç»“æ„ & ä»·æ ¼å¸¦åˆ†æï¼ˆåŸºäºèœå•é“¾æ¥ï¼‰
- ç»¼åˆæ‰€æœ‰èœå•æ–‡æœ¬ï¼ˆmenus.menu_textï¼‰ï¼Œä»ä¸‹é¢å‡ ä¸ªè§’åº¦åˆ†æï¼š
  - èœå•ç»“æ„æ˜¯å¦æ¸…æ™°ï¼ˆåˆ†ç±»æ˜¯å¦åˆç†ï¼šå‰èœ / ä¸»é£Ÿ / æ±¤å“ / é¥®å“ / ç”œå“ / å¥—é¤ç­‰ï¼‰
  - å“ç±»æ˜¯å¦å¤±è¡¡ï¼ˆä¾‹å¦‚ï¼šçƒ­èœè¿‡å¤šä½†æ— å†·ç›˜ï¼›æ²¡æœ‰å®¶åº­å¥—é¤ï¼›æ—  high-margin é¥®å“ç­‰ï¼‰
  - ä»·æ ¼å¸¦æ˜¯å¦åˆç†ï¼ˆæœ‰æ— æ˜æ˜¾æ–­å±‚ï¼›æ˜¯å¦ç¼ºä¹å…¥é—¨æ¬¾å’Œé«˜å®¢å•å‡çº§æ¬¾ï¼‰
- æŒ‡å‡º 3â€“7 ä¸ªå¯ä»¥æ”¹è¿›çš„ç‚¹ï¼Œä¾‹å¦‚ï¼š
  - å“ªäº›èœåº”è¯¥åšæˆç»„åˆå¥—é¤ / Family Meal
  - å“ªäº›èœé€‚åˆå½“åŠ ä»·å‡çº§é€‰é¡¹ï¼ˆä¾‹å¦‚ +$2 æ¢å¤§ä»½ / åŠ è‚‰ / åŠ é…èœï¼‰
  - å“ªäº›å“ç±»å®Œå…¨ç¼ºå¤±ä½†åœ¨è¯¥èœç³»/å•†åœˆæ˜¯åˆšéœ€ï¼ˆå¦‚å·èœé¦†å´æ²¡æœ‰å†·èœ/å†’èœç­‰ï¼‰

6ï¸âƒ£ å ‚é£Ÿ & å¤–å–æ”¶å…¥å¢é•¿æ‰“æ³•ï¼ˆç»“åˆæœç´¢é‡ä¸å®¢å•ä»·ï¼‰
- å·²çŸ¥å¤§è‡´å‡è®¾ï¼šæ¯ä¸ªæ ¸å¿ƒå…³é”®è¯æœˆæœç´¢é‡çº¦ {monthly_search_volume}ï¼Œå ‚é£Ÿå®¢å•ä»·çº¦ {dine_in_aov} ç¾å…ƒï¼Œå¤–å–å®¢å•ä»·çº¦ {delivery_aov} ç¾å…ƒã€‚
- è®¾è®¡ 2â€“3 å¥—ã€Œç®€å•ä½†æœ‰æ•ˆã€çš„å¢é•¿ç»„åˆæ‰“æ³•ï¼Œæ¯å¥—åŒ…æ‹¬ï¼š
  - ç›®æ ‡å®¢ç¾¤ï¼ˆå®¶åº­ã€åŠå…¬å®¤ç™½é¢†ã€é™„è¿‘å­¦ç”Ÿã€å‘¨æœ«èšé¤ç­‰ï¼‰
  - åœ¨ Google / å®˜ç½‘ / å„å¤–å–å¹³å°ä¸Šåˆ†åˆ«è¦åšçš„åŠ¨ä½œï¼ˆå¸¦æ“ä½œç»†èŠ‚ï¼‰
  - å¯¹åº”çš„èœå•åŠ¨ä½œï¼ˆä¾‹å¦‚æ–°å¢æŸç§å¥—é¤ã€æŠŠæŸäº›èœä¸Šç§»åˆ°é¦–å±ã€åš limited-time offer ç­‰ï¼‰
  - é¢„æœŸèƒ½æå‡çš„ç¯èŠ‚ï¼ˆæ›å…‰ã€ç‚¹å‡»ã€åŠ è´­ç‡ã€å¤è´­ç‡ç­‰ï¼‰

è¦æ±‚ï¼š
- è¯­æ°”åŠ¡å®ã€æ¥åœ°æ°”ï¼Œè®©æ¹¾åŒº / åŒ—ç¾åäººé¤å…è€æ¿èƒ½çœ‹å¾—æ‡‚ã€æ•¢ç…§åšï¼›
- ç»“æ„æ¸…æ™°ï¼Œç”¨å°æ ‡é¢˜ + åˆ—è¡¨ï¼›
- ä¸è¦ç»™ç©ºæ´çš„é¸¡æ±¤ï¼Œè¦ç»™å…·ä½“åŠ¨ä½œæ¸…å•ã€‚
"""

    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ]

    return call_llm_safe(messages)

# =========================
# ä¸»ç•Œé¢ï¼š1 åœ°å€ â†’ å€™é€‰é¤å…
# =========================

st.markdown("## 1ï¸âƒ£ è¾“å…¥é¤å…åœ°å€ï¼ˆè‡ªåŠ¨åŒ¹é…é™„è¿‘é¤å…ï¼‰")

address_input = st.text_input(
    "é¤å…åœ°å€ï¼ˆä¾‹å¦‚ï¼š1115 Clement St, San Francisco, CAï¼‰",
    "",
    help="å¯ä»¥æ˜¯å®Œæ•´åœ°å€æˆ–è¡—é“ + åŸå¸‚ï¼Œç³»ç»Ÿä¼šç”¨ Google è‡ªåŠ¨åŒ¹é…é™„è¿‘çš„é¤å…ã€‚",
)

if st.button("ğŸ” æ ¹æ®åœ°å€æŸ¥æ‰¾é™„è¿‘é¤å…"):
    if not address_input.strip():
        st.error("è¯·å…ˆè¾“å…¥åœ°å€ã€‚")
    else:
        with st.spinner("æ ¹æ®åœ°å€å®šä½å¹¶æŸ¥æ‰¾é™„è¿‘é¤å…..."):
            geocode_res = google_geocode(GOOGLE_API_KEY, address_input)
            if not geocode_res:
                st.error("æ— æ³•é€šè¿‡è¯¥åœ°å€æ‰¾åˆ°ä½ç½®ï¼Œè¯·æ£€æŸ¥æ‹¼å†™ã€‚")
            else:
                loc = geocode_res[0]["geometry"]["location"]
                lat = loc["lat"]
                lng = loc["lng"]
                nearby = google_places_nearby(
                    GOOGLE_API_KEY, lat, lng, radius_m=300, type_="restaurant"
                )
                if not nearby:
                    st.warning("é™„è¿‘ 300 ç±³å†…æœªæ‰¾åˆ°é¤å…ï¼Œè¯·å°è¯•è¾“å…¥æ›´ç²¾ç¡®çš„åœ°å€æˆ–æ”¾å¤§èŒƒå›´ã€‚")
                else:
                    st.session_state["candidate_places"] = nearby
                    st.session_state["run_analysis"] = False
                    st.success(f"å·²æ‰¾åˆ° {len(nearby)} å®¶é™„è¿‘é¤å…ï¼Œè¯·åœ¨ä¸‹æ–¹é€‰æ‹©ä½ çš„é¤å…ã€‚")

# =========================
# 2 é€‰æ‹©é¤å… + ä¸šåŠ¡å‚æ•° + èœå•é“¾æ¥
# =========================

candidate_places = st.session_state["candidate_places"]
selected_place_id: Optional[str] = None
place_label_list: List[str] = []

if candidate_places:
    st.markdown("### é€‰æ‹©ä½ çš„é¤å…")

    for p in candidate_places:
        label = f"{p.get('name', 'Unnamed')} â€” {p.get('vicinity', '')}"
        place_label_list.append(label)

    selected_index = st.selectbox(
        "åœ¨é™„è¿‘é¤å…åˆ—è¡¨ä¸­é€‰æ‹©ä½ è¦åˆ†æçš„é‚£ä¸€å®¶ï¼š",
        options=list(range(len(place_label_list))),
        format_func=lambda i: place_label_list[i],
        index=st.session_state.get("selected_index", 0),
    )
    st.session_state["selected_index"] = selected_index
    selected_place_id = candidate_places[selected_index]["place_id"]

    st.markdown("### å¡«å†™ä¸šåŠ¡å‚æ•°")

    col_aov1, col_aov2 = st.columns(2)
    with col_aov1:
        dine_in_aov = st.number_input(
            "å ‚é£Ÿå¹³å‡å®¢å•ä»·ï¼ˆUSDï¼‰",
            min_value=5.0,
            max_value=300.0,
            value=35.0,
            step=1.0,
        )
    with col_aov2:
        delivery_aov = st.number_input(
            "å¤–å–å¹³å‡å®¢å•ä»·ï¼ˆUSDï¼‰",
            min_value=5.0,
            max_value=300.0,
            value=45.0,
            step=1.0,
        )

    st.markdown("### å…³é”®è¯ & æœç´¢é‡ï¼ˆä¸æ‡‚å°±ç”¨é»˜è®¤å€¼ï¼‰")

    keywords_input = st.text_input(
        "æ ¸å¿ƒå…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰",
        "best chinese food, best asian food, best baked chicken",
        help="ç”¨äºä¼°ç®—ä½ åœ¨ Google æœ¬åœ°æœç´¢é‡Œçš„æœºä¼šã€‚ä¸æ‡‚å°±ç”¨é»˜è®¤å€¼ã€‚",
    )

    monthly_search_volume = st.number_input(
        "ä¼°ç®—æ¯ä¸ªæ ¸å¿ƒå…³é”®è¯çš„æœˆæœç´¢é‡ï¼ˆç»Ÿä¸€ç²—ç•¥å€¼ï¼‰",
        min_value=50,
        max_value=50000,
        value=500,
        step=50,
        help="ç®€å•ç†è§£ä¸ºï¼šè¿™ä¸€ç±»å…³é”®è¯å¤§æ¦‚æ¯æœˆæœ‰å¤šå°‘äººæœç´¢ã€‚",
    )

    website_override = st.text_input(
        "å¦‚æœä½ çš„å®˜ç½‘å’Œ Google é‡Œè®°å½•çš„ä¸ä¸€æ ·ï¼Œåœ¨è¿™é‡Œå¡«ä½ çš„å®˜ç½‘ URLï¼ˆå¯é€‰ï¼‰",
        "",
    )

    st.markdown("### èœå•é“¾æ¥ï¼ˆå¯é€‰ï¼Œä½†å¼ºçƒˆæ¨èï¼‰")
    menu_urls_input = st.text_area(
        "ç¬¬ä¸‰æ–¹å¤–å– / åœ¨çº¿ç‚¹é¤é“¾æ¥ï¼ˆå¦‚ DoorDash / UberEats / å®˜ç½‘ç‚¹é¤é¡µç­‰ï¼Œæ¯è¡Œä¸€ä¸ª URLï¼‰",
        "",
        height=120,
        help="ä¾‹å¦‚ï¼šhttps://www.doordash.com/store/xxx ...ï¼Œæ¯è¡Œä¸€ä¸ªé“¾æ¥ï¼Œç”¨äºåšèœå•çº§åˆ«åˆ†æã€‚",
    )

    if st.button("ğŸš€ è¿è¡Œåˆ†æ"):
        st.session_state["run_analysis"] = True
else:
    st.info("å…ˆè¾“å…¥åœ°å€å¹¶ç‚¹å‡»â€œæ ¹æ®åœ°å€æŸ¥æ‰¾é™„è¿‘é¤å…â€ã€‚")

# =========================
# 3 ä¸»åˆ†æé€»è¾‘
# =========================

if candidate_places and selected_place_id and st.session_state["run_analysis"]:
    # 1. è¯¦æƒ…
    with st.spinner("è·å–é¤å…è¯¦æƒ…ï¼ˆGoogle Place Detailsï¼‰..."):
        place_detail = google_place_details(GOOGLE_API_KEY, selected_place_id)

    st.success(f"å·²é”å®šé¤å…ï¼š**{place_detail.get('name', 'Unknown')}**")

    st.markdown("### ğŸ§¾ åŸºæœ¬ä¿¡æ¯ï¼ˆæ¥è‡ª Google Placesï¼‰")
    info_cols = st.columns(3)
    with info_cols[0]:
        st.write("**åç§°**:", place_detail.get("name"))
        st.write("**åœ°å€**:", place_detail.get("formatted_address"))
    with info_cols[1]:
        st.write("**ç”µè¯**:", place_detail.get("formatted_phone_number", "N/A"))
        st.write("**è¯„åˆ†**:", place_detail.get("rating", "N/A"))
        st.write("**è¯„è®ºæ•°**:", place_detail.get("user_ratings_total", "N/A"))
    with info_cols[2]:
        st.write("**ä»·æ ¼ç­‰çº§**:", place_detail.get("price_level", "N/A"))
        st.write("**å®˜ç½‘ï¼ˆGoogleï¼‰**:", place_detail.get("website", "N/A"))

    geometry = place_detail.get("geometry", {}).get("location", {})
    lat = geometry.get("lat")
    lng = geometry.get("lng")

    # 2. é™„è¿‘ç«äº‰å¯¹æ‰‹
    st.markdown("## 2ï¸âƒ£ é™„è¿‘ç«äº‰å¯¹æ‰‹ï¼ˆ3 å…¬é‡ŒèŒƒå›´ï¼‰")
    competitors_df = None
    competitors = []

    if lat is None or lng is None:
        st.warning("æœªèƒ½ä» Google è·å–ç»çº¬åº¦ï¼Œæ— æ³•æœç´¢é™„è¿‘ç«äº‰å¯¹æ‰‹ã€‚")
    else:
        with st.spinner("æœç´¢é™„è¿‘é¤å…ä½œä¸ºç«äº‰å¯¹æ‰‹..."):
            competitors = google_places_nearby(
                GOOGLE_API_KEY, lat, lng, radius_m=3000, type_="restaurant"
            )

        if competitors:
            comp_data = []
            for c in competitors:
                comp_data.append(
                    {
                        "Name": c.get("name"),
                        "Address": c.get("vicinity"),
                        "Rating": c.get("rating", None),
                        "Reviews": c.get("user_ratings_total", 0),
                    }
                )
            competitors_df = pd.DataFrame(comp_data)
            competitors_df = competitors_df[
                competitors_df["Name"].str.lower()
                != place_detail.get("name", "").lower()
            ]
            competitors_df = competitors_df.sort_values(
                by=["Rating", "Reviews"], ascending=[False, False]
            ).reset_index(drop=True)
            st.dataframe(competitors_df, use_container_width=True)
        else:
            st.info("æœªæ‰¾åˆ°ç«äº‰å¯¹æ‰‹ï¼ˆå¯èƒ½ API é™åˆ¶æˆ–é™„è¿‘é¤å…å¾ˆå°‘ï¼‰ã€‚")

    # 3. GBP è¯„åˆ†
    st.markdown("## 3ï¸âƒ£ Google å•†å®¶èµ„æ–™è¯„åˆ†ï¼ˆ40 åˆ†åˆ¶ï¼‰")
    gbp_result = score_gbp_profile(place_detail)
    st.metric("Google å•†å®¶èµ„æ–™å¾—åˆ†", f"{gbp_result['score']} / 40")

    gbp_rows = []
    for label, (pts, ok) in gbp_result["checks"].items():
        gbp_rows.append(
            {"æ£€æŸ¥é¡¹": label, "å¾—åˆ†": pts, "çŠ¶æ€": "âœ… å®Œæˆ" if ok else "âŒ ç¼ºå¤±/ä¸å®Œæ•´"}
        )
    st.table(pd.DataFrame(gbp_rows))

    # 4. ç½‘ç«™è¯„åˆ†
    st.markdown("## 4ï¸âƒ£ ç½‘ç«™å†…å®¹ & ä½“éªŒè¯„åˆ†ï¼ˆ40 åˆ†åˆ¶ï¼‰")
    effective_website = website_override or place_detail.get("website")

    if not effective_website:
        st.warning("æœªæä¾›ç½‘ç«™ URLï¼Œä¹Ÿæ— æ³•ä» Google è·å–ï¼Œç½‘ç«™è¯„åˆ†ä¸º 0ã€‚")
        web_result = {
            "score": 0,
            "checks": {"æ— ç½‘ç«™": (0, False)},
            "word_count": 0,
            "title": "",
            "text_snippet": "",
        }
    else:
        with st.spinner(f"æŠ“å–ç½‘ç«™ï¼š{effective_website}"):
            html = fetch_html(effective_website)
        web_result = score_website_basic(effective_website, html)

    st.metric("ç½‘ç«™å¾—åˆ†", f"{web_result['score']} / 40")
    web_rows = []
    for label, (pts, ok) in web_result["checks"].items():
        web_rows.append(
            {"æ£€æŸ¥é¡¹": label, "å¾—åˆ†": pts, "çŠ¶æ€": "âœ… æ˜¯" if ok else "âŒ å¦"}
        )
    st.table(pd.DataFrame(web_rows))

    # 5. å…³é”®è¯æ’å & å ‚é£Ÿ/å¤–å–è¥æ”¶æŸå¤±
    st.markdown("## 5ï¸âƒ£ å…³é”®è¯æ’å & å ‚é£Ÿ / å¤–å–æ½œåœ¨è¥æ”¶æŸå¤±")

    keywords = [k.strip() for k in keywords_input.split(",") if k.strip()]
    rank_results: List[Dict[str, Any]] = []

    if not keywords:
        st.info("æœªæä¾›å…³é”®è¯ï¼Œè·³è¿‡æ’åæ¨¡æ‹Ÿå’Œè¥æ”¶ä¼°ç®—ã€‚")
    else:
        for kw in keywords:
            st.write(f"### å…³é”®è¯ï¼š**{kw}**")
            rank_bucket = "none"
            rank_position = None

            if SERPAPI_KEY and lat is not None and lng is not None:
                with st.spinner(f"ä½¿ç”¨ SerpAPI æŸ¥è¯¢ Google Maps æ’åï¼š{kw}"):
                    try:
                        serp_json = serpapi_google_maps_search(
                            SERPAPI_KEY, kw, lat, lng
                        )
                        rank_position = infer_rank_from_serpapi(
                            serp_json, place_detail.get("name", "")
                        )
                        if rank_position is not None:
                            if rank_position <= 3:
                                rank_bucket = "top3"
                            elif rank_position <= 10:
                                rank_bucket = "4-10"
                            else:
                                rank_bucket = "none"
                    except Exception as e:
                        st.warning(f"SerpAPI æŸ¥è¯¢å‡ºé”™ï¼š{e}")
                        rank_bucket = "none"
                        rank_position = None
            else:
                # æ²¡æœ‰ SerpAPIï¼šç”¨è¯„åˆ†+è¯„è®ºç®€å•è¿‘ä¼¼æ’åº
                if competitors:
                    all_places = competitors + [place_detail]
                    all_places_data = []
                    for p in all_places:
                        all_places_data.append(
                            {
                                "name": p.get("name", ""),
                                "rating": p.get("rating", 0),
                                "reviews": p.get("user_ratings_total", 0),
                            }
                        )
                    df_all = pd.DataFrame(all_places_data)
                    df_all["score"] = (
                        df_all["rating"].fillna(0) * 10
                        + df_all["reviews"].fillna(0) / 10
                    )
                    df_all = df_all.sort_values(
                        by="score", ascending=False
                    ).reset_index(drop=True)
                    positions = df_all["name"].str.lower().tolist()
                    name_lower = place_detail.get("name", "").lower()
                    if name_lower in positions:
                        pos = positions.index(name_lower) + 1
                        rank_position = pos
                        if pos <= 3:
                            rank_bucket = "top3"
                        elif pos <= 10:
                            rank_bucket = "4-10"
                        else:
                            rank_bucket = "none"

            monthly_loss_dine_in = estimate_revenue_loss(
                monthly_search_volume,
                rank_bucket,
                dine_in_aov,
                channel="dine-in",
            )
            monthly_loss_delivery = estimate_revenue_loss(
                monthly_search_volume,
                rank_bucket,
                delivery_aov,
                channel="delivery",
            )

            st.write(
                f"- ä¼°è®¡å½“å‰æ’åï¼š"
                f"{'Top 3' if rank_bucket=='top3' else ('ç¬¬ 4â€“10 å' if rank_bucket=='4-10' else 'æœªè¿›å…¥å‰ 10')}"
                f"{'' if rank_position is None else f'ï¼ˆæ¨æµ‹åæ¬¡ï¼š{rank_position}ï¼‰'}"
            )
            st.write(
                f"- å ‚é£Ÿï¼šæ¯æœˆå¯èƒ½å°‘èµšçº¦ **${monthly_loss_dine_in:,.0f}**ï¼›"
                f"å¤–å–ï¼šæ¯æœˆå¯èƒ½å°‘èµšçº¦ **${monthly_loss_delivery:,.0f}**ã€‚"
            )

            rank_results.append(
                {
                    "å…³é”®è¯": kw,
                    "é¢„ä¼°åæ¬¡": rank_position,
                    "åæ¬¡åŒºé—´": rank_bucket,
                    "å ‚é£ŸæœˆæŸå¤±($)": round(monthly_loss_dine_in, 2),
                    "å¤–å–æœˆæŸå¤±($)": round(monthly_loss_delivery, 2),
                }
            )

        if rank_results:
            st.markdown("#### å…³é”®è¯ & å ‚é£Ÿ/å¤–å–è¥æ”¶æŸå¤±æ±‡æ€»")
            st.dataframe(pd.DataFrame(rank_results), use_container_width=True)

    # 6. ç»¼åˆå¾—åˆ†
    st.markdown("## 6ï¸âƒ£ æ€»ä½“åœ¨çº¿å¥åº·æ€»ç»“")
    total_score = gbp_result["score"] + web_result["score"]
    st.metric("ç»¼åˆå¾—åˆ†ï¼ˆProfile + Websiteï¼‰", f"{total_score} / 80")
    st.write(
        "- **40 åˆ†ä»¥ä¸‹**ï¼šåœ¨çº¿åŸºç¡€éå¸¸å¼±ï¼ŒåŸºæœ¬å±äº â€œPoorâ€ã€‚\n"
        "- **40â€“60 åˆ†**ï¼šä¸­ç­‰ï¼Œèƒ½è¢«æ‰¾å¾—åˆ°ï¼Œä½†ä¸å ä¼˜åŠ¿ã€‚\n"
        "- **60 åˆ†ä»¥ä¸Š**ï¼šç›¸å¯¹å¥åº·ï¼Œå¯ä»¥å¼€å§‹ç©ç²¾ç»†åŒ–è¿è¥å’Œæ´»åŠ¨ã€‚\n"
    )

    # 7. ChatGPT æ·±åº¦åˆ†æï¼ˆå«èœå•ï¼‰
    st.markdown("## 7ï¸âƒ£ ChatGPT å¤šç»´èœç³» & èœå•ç»“æ„ & è¿è¥åˆ†æ")

    menus: List[Dict[str, str]] = []
    if menu_urls_input.strip():
        menu_urls = [u.strip() for u in menu_urls_input.splitlines() if u.strip()]
        with st.spinner("æŠ“å–èœå•é¡µé¢å¹¶æå–èœå“æ–‡æœ¬..."):
            menus = build_menu_payload(menu_urls)

        if menus:
            st.markdown("#### èœå•æŠ“å–é¢„è§ˆï¼ˆè°ƒè¯•ç”¨ï¼‰")
            preview_rows = []
            for m in menus:
                preview_rows.append(
                    {
                        "æ¥æº": m.get("source", ""),
                        "URL": m.get("url", ""),
                        "çŠ¶æ€": m.get("status", ""),
                        "èœå•æ–‡æœ¬é¢„è§ˆ": (m.get("menu_text") or "")[:120],
                    }
                )
            st.dataframe(pd.DataFrame(preview_rows), use_container_width=True)
    else:
        st.info("æœªå¡«å†™ä»»ä½•èœå•é“¾æ¥ï¼Œå°†åªåšçº¿ä¸Šæ›å…‰å’Œç½‘ç«™å±‚é¢çš„åˆ†æã€‚")

    ai_report = None
    if not OPENAI_API_KEY:
        st.warning("æœªé…ç½® OPENAI_API_KEYï¼Œå¦‚éœ€ AI æ·±åº¦åˆ†æè¯·åœ¨ Secrets ä¸­æ·»åŠ ã€‚")
    else:
        if st.button("ğŸ¤– ç”Ÿæˆ AI æ·±åº¦åˆ†ææŠ¥å‘Š"):
            with st.spinner("æ­£åœ¨è°ƒç”¨ ChatGPT åˆ†æ..."):
                ai_report = llm_deep_analysis(
                    place_detail=place_detail,
                    gbp_result=gbp_result,
                    web_result=web_result,
                    competitors_df=competitors_df,
                    rank_results=rank_results,
                    monthly_search_volume=monthly_search_volume,
                    dine_in_aov=dine_in_aov,
                    delivery_aov=delivery_aov,
                    menus=menus,
                )
                st.markdown(ai_report)

    # 8. CTAï¼šå…è´¹è·å–å®Œæ•´æŠ¥å‘Šï¼ˆWhatsAppï¼‰
    st.markdown("## 8ï¸âƒ£ å…è´¹è·å–å®Œæ•´ Aurainsight æŠ¥å‘Š")

    st.markdown(
         
<div style="margin-top:10px; margin-bottom:20px;">
  <a href="https
