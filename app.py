import os
import json
from typing import List, Dict, Any, Optional

import streamlit as st
import pandas as pd
import requests
import googlemaps
from bs4 import BeautifulSoup
from urllib.parse import urlparse

from openai import OpenAI

# =========================
# åŸºæœ¬é…ç½® & Secrets
# =========================
st.set_page_config(
    page_title="Restaurant Local SEO & Competitor Analyzer",
    layout="wide",
)

# ---- ä» Streamlit Secrets è¯»å– API å¯†é’¥ ----
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY", "")
SERPAPI_KEY = st.secrets.get("SERPAPI_KEY", "")
YELP_API_KEY = st.secrets.get("YELP_API_KEY", "")
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")

if not GOOGLE_API_KEY:
    st.error("ç¼ºå°‘ GOOGLE_API_KEYï¼Œè¯·å…ˆåœ¨ Streamlit Secrets ä¸­é…ç½®åå†åˆ·æ–°ã€‚")
    st.stop()

# é…ç½® OpenAI å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
client: Optional[OpenAI] = None
if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)

# =========================
# é¡µé¢æ ‡é¢˜
# =========================
st.title("ğŸœ Restaurant Local SEO & Competitor Analyzer")
st.write(
    "å¤åˆ» Owner.com é£æ ¼çš„é¤å…åœ¨çº¿å¥åº·æ£€æµ‹ + ChatGPT æ·±åº¦åˆ†æï¼š\n"
    "- è‡ªåŠ¨è¯†åˆ«é™„è¿‘ç«äº‰å¯¹æ‰‹\n"
    "- è¯„ä¼° Google å•†å®¶èµ„æ–™å®Œæ•´åº¦ï¼ˆ40åˆ†ï¼‰\n"
    "- æ£€æŸ¥ç½‘ç«™åŸºç¡€ SEO / å†…å®¹ï¼ˆ40åˆ†ï¼‰\n"
    "- æ¨¡æ‹Ÿæœ¬åœ°æœç´¢æ’å + ç²—ç•¥è¥æ”¶æŸå¤±\n"
    "- ä½¿ç”¨ ChatGPT å¯¹èœç³»ç»†åˆ†ï¼ˆç²¤/å·/æ²ª/ä¸œåŒ—/èŒ¶é¤å…ç­‰ï¼‰ä¸è¿è¥ç»™å‡ºå¤šç»´åˆ†æ"
)

# =========================
# ä¾§è¾¹æ ï¼šä¸šåŠ¡å‚æ•°ï¼ˆéå¯†é’¥ï¼‰
# =========================
st.sidebar.header("ğŸ“Š åˆ†æå‚æ•°")

default_radius_km = st.sidebar.slider(
    "ç«äº‰å¯¹æ‰‹æœç´¢åŠå¾„ï¼ˆå…¬é‡Œï¼‰", 0.5, 10.0, 3.0, 0.5
)

avg_order_value = st.sidebar.number_input(
    "å¹³å‡å®¢å•ä»·ï¼ˆUSDï¼‰", min_value=5.0, max_value=200.0, value=40.0, step=1.0
)
assumed_ctr = st.sidebar.slider(
    "ç‚¹å‡»ç‡å‡è®¾ï¼ˆç”¨æˆ·çœ‹åˆ°ä½ åä¼šç‚¹è¿›èµ„æ–™/ç½‘ç«™çš„æ¯”ä¾‹ï¼‰",
    0.05, 0.5, 0.15, 0.01
)
assumed_conv = st.sidebar.slider(
    "ä¸‹å•è½¬åŒ–ç‡å‡è®¾ï¼ˆç‚¹è¿›æ¥åä¸‹å•çš„æ¯”ä¾‹ï¼‰",
    0.05, 0.5, 0.20, 0.01
)

st.sidebar.caption("ä¸Šé¢ä¸‰é¡¹åªç”¨äºç²—ç•¥ä¼°ç®—æ½œåœ¨è¥æ”¶æŸå¤±ï¼Œå¯æ ¹æ®å®é™…è°ƒæ•´ã€‚")

# =========================
# å·¥å…·å‡½æ•°ï¼ˆå¸¦ç¼“å­˜ï¼‰
# =========================

@st.cache_data(show_spinner=False)
def gm_client(key: str):
    return googlemaps.Client(key=key)

@st.cache_data(show_spinner=False)
def google_places_search(api_key: str, query: str) -> List[Dict[str, Any]]:
    gmaps = gm_client(api_key)
    result = gmaps.places(query=query)
    return result.get("results", [])

@st.cache_data(show_spinner=False)
def google_place_details(api_key: str, place_id: str) -> Dict[str, Any]:
    gmaps = gm_client(api_key)
    fields = [
        "name",
        "formatted_address",
        "formatted_phone_number",
        "geometry",
        "rating",
        "user_ratings_total",
        "types",
        "opening_hours",
        "website",
        "price_level",
        "photos",
    ]
    result = gmaps.place(place_id=place_id, fields=fields)
    return result.get("result", {})

@st.cache_data(show_spinner=False)
def google_places_nearby(
    api_key: str, lat: float, lng: float, radius_m: int, type_: str = "restaurant"
) -> List[Dict[str, Any]]:
    gmaps = gm_client(api_key)
    result = gmaps.places_nearby(
        location=(lat, lng), radius=radius_m, type=type_
    )
    return result.get("results", [])

@st.cache_data(show_spinner=False)
def fetch_html(url: str) -> Optional[str]:
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Restaurant-Analyzer)"}
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            return resp.text
        return None
    except Exception:
        return None

@st.cache_data(show_spinner=False)
def serpapi_google_maps_search(
    serpapi_key: str, query: str, lat: float, lng: float, zoom: float = 13.0
) -> Dict[str, Any]:
    url = "https://serpapi.com/search"
    ll_param = f"@{lat},{lng},{zoom}z"
    params = {
        "engine": "google_maps",
        "type": "search",
        "q": query,
        "ll": ll_param,
        "api_key": serpapi_key,
    }
    resp = requests.get(url, params=params, timeout=30)
    resp.raise_for_status()
    return resp.json()

# =========================
# è¯„åˆ†å‡½æ•°
# =========================

def score_gbp_profile(place: Dict[str, Any]) -> Dict[str, Any]:
    """ç®€åŒ–ç‰ˆ Google å•†å®¶èµ„æ–™è¯„åˆ†ï¼Œæ€»åˆ† 40 åˆ†ã€‚"""
    score = 0
    checks = {}

    # 1. åç§° & åœ°å€
    has_name = bool(place.get("name"))
    has_address = bool(place.get("formatted_address"))
    pts = 4 if (has_name and has_address) else 0
    score += pts
    checks["åç§°/åœ°å€å®Œæ•´"] = (pts, has_name and has_address)

    # 2. ç”µè¯
    has_phone = bool(place.get("formatted_phone_number"))
    pts = 4 if has_phone else 0
    score += pts
    checks["ç”µè¯"] = (pts, has_phone)

    # 3. è¥ä¸šæ—¶é—´
    opening_hours = place.get("opening_hours", {})
    has_hours = bool(opening_hours.get("weekday_text")) or opening_hours.get(
        "open_now"
    ) is not None
    pts = 4 if has_hours else 0
    score += pts
    checks["è¥ä¸šæ—¶é—´"] = (pts, has_hours)

    # 4. ç½‘ç«™
    has_website = bool(place.get("website"))
    pts = 4 if has_website else 0
    score += pts
    checks["ç½‘ç«™é“¾æ¥"] = (pts, has_website)

    # 5. è¯„åˆ† & è¯„è®ºæ•°
    rating = place.get("rating")
    reviews = place.get("user_ratings_total", 0)
    has_reviews = rating is not None and reviews >= 10
    pts = 6 if has_reviews else 0
    score += pts
    checks["è¯„åˆ† & â‰¥10æ¡è¯„è®º"] = (pts, has_reviews)

    # 6. ç±»åˆ«
    types_ = place.get("types", [])
    has_category = any(t for t in types_ if t != "point_of_interest")
    pts = 6 if has_category else 0
    score += pts
    checks["ç±»åˆ«è®¾ç½®"] = (pts, has_category)

    # 7. ä»·æ ¼ç­‰çº§
    has_price_level = place.get("price_level") is not None
    pts = 4 if has_price_level else 0
    score += pts
    checks["ä»·æ ¼åŒºé—´"] = (pts, has_price_level)

    # 8. ç…§ç‰‡
    photos = place.get("photos", [])
    has_photos = len(photos) > 0
    pts = 8 if has_photos else 0
    score += pts
    checks["ç…§ç‰‡/å›¾ç‰‡"] = (pts, has_photos)

    return {"score": score, "checks": checks}

def score_website_basic(url: str, html: Optional[str]) -> Dict[str, Any]:
    """ç®€åŒ–ç‰ˆç½‘ç«™è¯„åˆ†ï¼Œæ€»åˆ† 40 åˆ† + è¿”å›æ–‡æœ¬æ‘˜è¦ã€‚"""
    if not url or not html:
        return {
            "score": 0,
            "checks": {"æ— æ³•è®¿é—®ç½‘ç«™": (0, False)},
            "word_count": 0,
            "title": "",
            "text_snippet": "",
        }

    soup = BeautifulSoup(html, "lxml")
    score = 0
    checks = {}

    # å…¨ç«™æ–‡æœ¬
    texts = soup.get_text(separator=" ", strip=True)
    word_count = len(texts.split())
    text_snippet = texts[:3000]  # ä¼ ç»™ ChatGPT ç”¨

    # 1. Title
    title = soup.title.string.strip() if soup.title and soup.title.string else ""
    has_title = bool(title)
    pts = 6 if has_title else 0
    score += pts
    checks["æœ‰é¡µé¢æ ‡é¢˜ï¼ˆtitleï¼‰"] = (pts, has_title)

    # 2. Meta Description
    desc_tag = soup.find("meta", attrs={"name": "description"})
    has_desc = bool(desc_tag and desc_tag.get("content"))
    pts = 6 if has_desc else 0
    score += pts
    checks["æœ‰ Meta Description"] = (pts, has_desc)

    # 3. H1
    h1 = soup.find("h1")
    has_h1 = bool(h1 and h1.get_text(strip=True))
    pts = 4 if has_h1 else 0
    score += pts
    checks["æœ‰ H1 æ ‡é¢˜"] = (pts, has_h1)

    # 4. æ–‡æœ¬æ€»é‡
    has_sufficient_text = word_count >= 300
    pts = 8 if has_sufficient_text else 0
    score += pts
    checks["æ–‡æœ¬é‡ â‰¥ 300 è¯"] = (pts, has_sufficient_text)

    # 5. è”ç³»æ–¹å¼
    has_phone_text = any(x in texts for x in ["(", ")", "-", "+1"])
    pts = 4 if has_phone_text else 0
    score += pts
    checks["é¡µé¢ä¸Šèƒ½çœ‹åˆ°ç”µè¯"] = (pts, has_phone_text)

    # 6. èœå“/é¤å…å…³é”®è¯ï¼ˆç®€å•åŒ¹é…ï¼‰
    keywords = ["chinese", "cantonese", "szechuan", "sichuan", "shanghai",
                "noodle", "rice", "dumpling", "hot pot", "bbq", "dim sum"]
    kw_hit = any(kw.lower() in texts.lower() for kw in keywords)
    pts = 6 if kw_hit else 0
    score += pts
    checks["æ–‡æœ¬åŒ…å«èœå“/èœç³»å…³é”®è¯"] = (pts, kw_hit)

    # 7. https
    parsed = urlparse(url)
    has_https = parsed.scheme == "https"
    pts = 6 if has_https else 0
    score += pts
    checks["ä½¿ç”¨ HTTPS"] = (pts, has_https)

    return {
        "score": score,
        "checks": checks,
        "word_count": word_count,
        "title": title,
        "text_snippet": text_snippet,
    }

def estimate_revenue_loss(
    monthly_search_volume: int,
    rank_bucket: str,
    avg_order_value: float,
    ctr: float,
    conv: float,
) -> float:
    """ç²—ç•¥è¥æ”¶æŸå¤±ä¼°ç®—ã€‚"""
    ideal_customers = monthly_search_volume * ctr * conv
    if rank_bucket == "top3":
        current_factor = 1.0
    elif rank_bucket == "4-10":
        current_factor = 0.4
    else:
        current_factor = 0.1

    current_customers = ideal_customers * current_factor
    potential_extra_customers = ideal_customers - current_customers
    monthly_loss = potential_extra_customers * avg_order_value
    return monthly_loss

def infer_rank_from_serpapi(
    serp_json: Dict[str, Any], business_name: str
) -> Optional[int]:
    """ä» SerpAPI Google Maps ç»“æœä¸­æ‰¾åˆ°å½“å‰é¤å…åæ¬¡ã€‚"""
    results = serp_json.get("local_results") or serp_json.get("places_results") or []
    for idx, res in enumerate(results, start=1):
        name = res.get("title") or res.get("name", "")
        if name and business_name.lower() in name.lower():
            return idx
    return None

# =========================
# ChatGPT æ·±åº¦åˆ†æå‡½æ•°
# =========================

def llm_deep_analysis(
    place_detail: Dict[str, Any],
    gbp_result: Dict[str, Any],
    web_result: Dict[str, Any],
    competitors_df: Optional[pd.DataFrame],
    rank_results: List[Dict[str, Any]],
    monthly_search_volume: int,
    avg_order_value: float,
) -> str:
    if client is None:
        return "æœªé…ç½® OPENAI_API_KEYï¼Œæ— æ³•è°ƒç”¨ ChatGPTã€‚"

    # åªå–å‰ 5 ä¸ªç«äº‰å¯¹æ‰‹ï¼Œé¿å… prompt å¤ªé•¿
    comp_json = []
    if competitors_df is not None and not competitors_df.empty:
        sub = competitors_df.head(5)
        comp_json = sub.to_dict(orient="records")

    payload = {
        "restaurant": {
            "name": place_detail.get("name"),
            "address": place_detail.get("formatted_address"),
            "phone": place_detail.get("formatted_phone_number"),
            "types": place_detail.get("types", []),
            "rating": place_detail.get("rating"),
            "reviews": place_detail.get("user_ratings_total"),
            "price_level": place_detail.get("price_level"),
        },
        "gbp_score": gbp_result["score"],
        "gbp_checks": gbp_result["checks"],
        "website_score": web_result["score"],
        "website_title": web_result.get("title", ""),
        "website_word_count": web_result.get("word_count", 0),
        "competitors": comp_json,
        "rank_results": rank_results,
        "assumptions": {
            "monthly_search_volume_per_keyword": monthly_search_volume,
            "avg_order_value": avg_order_value,
        },
    }

    text_snippet = web_result.get("text_snippet", "")

    system_msg = (
        "ä½ æ˜¯ä¸€åä¸“é—¨æœåŠ¡åŒ—ç¾é¤é¦†çš„æœ¬åœ°è¥é”€å’Œå¤–å–è¿è¥é¡¾é—®ï¼Œæ›¾ä»»èŒäºéº¦è‚¯é”¡ä¸€ä¸ªä¸“é—¨åšé¤é¥®åˆ†æçš„éƒ¨é—¨"
        "éå¸¸äº†è§£ä¸–ç•Œå„åœ°çš„èœç³»ï¼Œå°¤å…¶åœ¨ä¸­é¤èœç³»çš„ç»†åˆ†é¢†åŸŸå±äºè¡Œä¸šæƒå¨ï¼Œå¦‚ç²¤èœã€èŒ¶é¤å…ã€å·èœã€æ¹˜èœã€ä¸œåŒ—èœã€ä¸Šæµ·èœç­‰ç»†åˆ†èœç³»ï¼Œ"
        "ç†Ÿæ‚‰ Google æœ¬åœ°æœç´¢å’Œ UberEats/DoorDash/Grubhub/Hungrypanda/Fantuan ç­‰å¹³å°çš„è¿è¥é€»è¾‘ã€‚"
        "è¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ï¼Œä½†åœ¨éœ€è¦æ—¶å¯åŠ å°‘é‡è‹±æ–‡æœ¯è¯­ã€‚"
    )

    user_msg = f"""
è¿™æ˜¯ä¸€ä¸ªé¤å…çš„åœ¨çº¿æ•°æ®ï¼Œè¯·ä½ åš**å¤šç»´æ·±åº¦åˆ†æ**å¹¶ç»™å‡ºç»†åˆ†èœç³»åˆ¤æ–­ä¸è¿è¥å»ºè®®ã€‚

ã€ç»“æ„åŒ–æ•°æ® JSONã€‘
{json.dumps(payload, ensure_ascii=False, indent=2)}

ã€ç½‘ç«™æ–‡æœ¬ç‰‡æ®µï¼ˆæœ€å¤š 3000 å­—ç¬¦ï¼‰ã€‘
{text_snippet}

è¯·ä½ å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. **èœç³»ç»†åˆ†åˆ¤æ–­**
   - æ ¹æ®é¤å…åç§°ã€ç½‘ç«™æ–‡æœ¬ã€Google ç±»å‹ç­‰ï¼Œåˆ¤æ–­å®ƒæ›´åƒï¼šç²¤èœé¦†ï¼ŸèŒ¶é¤å…ï¼Ÿå·èœï¼Ÿä¸œåŒ—èœï¼Ÿä¸Šæµ·èœï¼Ÿèåˆäºšæ´²ï¼Ÿå…¶ä»–ï¼Ÿ
   - ç»™å‡º 1â€“2 å¥ç†ç”±ï¼Œå¹¶ç”¨ä¸­æ–‡ç»™å‡º 1 ä¸ªæœ€åˆé€‚çš„èœç³»æ ‡ç­¾ï¼ˆå¦‚ï¼š`æ­£å®—å·èœé¦†`ã€`æ¸¯å¼èŒ¶é¤å…`ï¼‰ã€‚

2. **æœ¬åœ°ç«äº‰æ ¼å±€åˆ†æ**
   - æ ¹æ®ç«äº‰å¯¹æ‰‹åˆ—è¡¨ï¼Œåˆ¤æ–­ï¼šä»–ä»¬ä¸»è¦æ˜¯å“ªå‡ ç±»é¤å…ï¼ˆä¾‹ï¼šMr Szechuan = å·èœï¼ŒKhao Tiew = æ³°å›½èœç­‰ï¼‰ã€‚
   - ç®€è¦è¯´æ˜ï¼šå½“å‰è¿™å®¶é¤å…åœ¨â€œä»·æ ¼å¸¦ã€è¯„åˆ†ã€è¯„è®ºé‡ã€å“ç‰Œè®°å¿†ç‚¹â€ä¸Šç›¸æ¯”ç«äº‰å¯¹æ‰‹çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚

3. **Google å•†å®¶èµ„æ–™ä¼˜åŒ–å»ºè®®ï¼ˆGBPï¼‰**
   - æ ¹æ® gbp_score å’Œ checksï¼Œåˆ—å‡ºæœ€ä¼˜å…ˆéœ€è¦è¡¥é½çš„ 3â€“5 é¡¹ï¼ˆä¾‹å¦‚ï¼šä¸Šä¼ æ›´å¤šèœå“ç…§ç‰‡ã€è¡¥å……è¥ä¸šæ—¶é—´ã€å¢åŠ æœåŠ¡é€‰é¡¹ç­‰ï¼‰ã€‚
   - å¯¹æ¯ä¸€é¡¹ç»™å‡ºå…·ä½“æ‰§è¡Œå»ºè®®ï¼ˆè¦å†™å¾—åƒä½ è¦è·Ÿè€æ¿è§£é‡Šï¼Œâ€œä¸ºä»€ä¹ˆåšè¿™ä»¶äº‹ä¼šå¤šå¸¦æ¥è®¢å•â€ï¼‰ã€‚

4. **ç½‘ç«™å†…å®¹ä¸è½¬åŒ–å»ºè®®**
   - æ ¹æ® website_scoreã€word_count å’Œç½‘ç«™æ–‡æœ¬ç‰‡æ®µï¼ŒæŒ‡å‡ºç›®å‰ç½‘ç«™å†…å®¹åœ¨ä»¥ä¸‹å‡ ä¸ªç»´åº¦æ˜¯å¦è¾¾æ ‡ï¼š
     - æ˜¯å¦æ¸…æ™°è¯´æ˜èœç³»å’Œæ‹›ç‰Œèœï¼Ÿ
     - æ˜¯å¦æœ‰è¶³å¤Ÿæ–‡æœ¬æ”¯æ’‘ SEOï¼Ÿ
     - æ˜¯å¦æœ‰å¼ºçš„åœ¨çº¿ä¸‹å•/é¢„è®¢ CTAï¼Ÿ
   - ç»™å‡º 3â€“5 æ¡å…·ä½“å»ºè®®ï¼ŒåŒ…å«ï¼šåº”è¯¥å¢åŠ ä»€ä¹ˆæ¿å—ï¼ˆä¾‹å¦‚ï¼šæ‹›ç‰Œèœä»‹ç»ã€åˆå¸‚å¥—é¤ã€å®¶åº­èšä¼š/å®´ä¼šé¡µé¢ç­‰ï¼‰ã€éœ€è¦åŠ å…¥å“ªäº›å…³é”®è¯ã€‚

5. **å¤–å–ä¸æœ¬åœ°æœç´¢å¢é•¿ç­–ç•¥**
   - ç»“åˆ rank_results çš„å…³é”®è¯å’Œä½ å¯¹èœç³»çš„åˆ¤æ–­ï¼Œç»™å‡º 3 æ¡â€œæ”»å  Google æœç´¢ + å¤–å–å¹³å°â€çš„ç»„åˆæ‰“æ³•ã€‚
   - æ¯æ¡æ‰“æ³•éƒ½è¦åŒ…å«ï¼š
     - ç›®æ ‡å…³é”®è¯ï¼ˆä¸­è‹±éƒ½å¯ä»¥ï¼‰
     - åœ¨ Google å•†å®¶ã€ç½‘ç«™ã€å¤–å–å¹³å°å„è‡ªè¦åšä»€ä¹ˆè°ƒæ•´
     - é¢„æœŸä¼šå¸¦æ¥æ€æ ·ç±»å‹çš„å®¢äººï¼ˆå®¶åº­èšé¤ã€åŠå…¬å®¤åˆé¤ã€å­¦ç”Ÿå¤œå®µç­‰ï¼‰ã€‚

è¦æ±‚ï¼š
- ç”¨å°æ ‡é¢˜ + åˆ—è¡¨çš„æ–¹å¼è¾“å‡ºï¼Œæ–¹ä¾¿å¤åˆ¶åˆ°æŠ¥å‘Šé‡Œã€‚
- è¯­æ°”ä¸“ä¸šä½†æ¥åœ°æ°”ï¼Œé¢å‘æ¹¾åŒº/åŒ—ç¾åäººé¤å…è€æ¿ã€‚
"""

    completion = client.chat.completions.create(
        model="gpt-4.1-mini",   # æˆ– gpt-4o-mini / gpt-4.1ï¼Œçœ‹ä½ è´¦å·æƒé™
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.4,
    )

    return completion.choices[0].message.content


# =========================
# ä¸»ç•Œé¢äº¤äº’
# =========================

st.markdown("## 1ï¸âƒ£ è¾“å…¥é¤å…ä¿¡æ¯")

col1, col2 = st.columns(2)
with col1:
    restaurant_name = st.text_input("é¤å…åç§°ï¼ˆRestaurant Nameï¼‰", "")
with col2:
    city_region = st.text_input("åŸå¸‚/åŒºåŸŸï¼ˆå¦‚ï¼šOuter Sunset, San Franciscoï¼‰", "")

website_url = st.text_input(
    "é¤å…å®˜ç½‘ URLï¼ˆå¯ç•™ç©ºï¼Œä¼˜å…ˆä½¿ç”¨ Google å•†å®¶é‡Œè®°å½•çš„å®˜ç½‘ï¼‰",
    "",
)

keywords_input = st.text_input(
    "æ ¸å¿ƒå…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼Œä¾‹å¦‚ï¼šbest chinese food outer sunset, best asian food west portalï¼‰",
    "best chinese food outer sunset, best asian food outer sunset",
)

monthly_search_volume = st.number_input(
    "ä¼°ç®—æ¯ä¸ªæ ¸å¿ƒå…³é”®è¯çš„æœˆæœç´¢é‡ï¼ˆç»Ÿä¸€ç²—ç•¥å€¼ï¼‰",
    min_value=50,
    max_value=50000,
    value=500,
    step=50,
)

run_btn = st.button("ğŸš€ è¿è¡Œåˆ†æ")

if run_btn:
    if not restaurant_name or not city_region:
        st.error("è¯·å¡«å†™é¤å…åç§°å’ŒåŸå¸‚/åŒºåŸŸã€‚")
        st.stop()

    query = f"{restaurant_name} {city_region}"
    with st.spinner(f"åœ¨ Google Places ä¸­æœç´¢ï¼š{query}"):
        places = google_places_search(GOOGLE_API_KEY, query)

    if not places:
        st.error("Google Places æœªæ‰¾åˆ°åŒ¹é…é¤å…ï¼Œè¯·æ£€æŸ¥åç§°å’ŒåŸå¸‚ã€‚")
        st.stop()

    target = places[0]
    place_id = target["place_id"]

    with st.spinner("è·å–é¤å…è¯¦æƒ…ï¼ˆGoogle Place Detailsï¼‰..."):
        place_detail = google_place_details(GOOGLE_API_KEY, place_id)

    st.success(f"å·²æ‰¾åˆ°é¤å…ï¼š**{place_detail.get('name', 'Unknown')}**")

    # ---- åŸºç¡€ä¿¡æ¯ ----
    st.markdown("### ğŸ§¾ åŸºæœ¬ä¿¡æ¯ï¼ˆæ¥è‡ª Google Placesï¼‰")
    info_cols = st.columns(3)
    with info_cols[0]:
        st.write("**åç§°**:", place_detail.get("name"))
        st.write("**åœ°å€**:", place_detail.get("formatted_address"))
    with info_cols[1]:
        st.write("**ç”µè¯**:", place_detail.get("formatted_phone_number", "N/A"))
        st.write("**è¯„åˆ†**:", place_detail.get("rating", "N/A"))
        st.write("**è¯„è®ºæ•°**:", place_detail.get("user_ratings_total", "N/A"))
    with info_cols[2]:
        st.write("**ä»·æ ¼ç­‰çº§**:", place_detail.get("price_level", "N/A"))
        st.write("**å®˜ç½‘ï¼ˆGoogleï¼‰**:", place_detail.get("website", "N/A"))

    geometry = place_detail.get("geometry", {}).get("location", {})
    lat = geometry.get("lat")
    lng = geometry.get("lng")

    # ---- ç«äº‰å¯¹æ‰‹ ----
    st.markdown("## 2ï¸âƒ£ é™„è¿‘ç«äº‰å¯¹æ‰‹ï¼ˆGoogle Places Nearbyï¼‰")
    competitors_df = None
    competitors = []

    if lat is None or lng is None:
        st.warning("æœªèƒ½ä» Google è·å–ç»çº¬åº¦ï¼Œæ— æ³•æœç´¢é™„è¿‘ç«äº‰å¯¹æ‰‹ã€‚")
    else:
        radius_m = int(default_radius_km * 1000)
        with st.spinner("æœç´¢é™„è¿‘é¤å…ä½œä¸ºç«äº‰å¯¹æ‰‹..."):
            competitors = google_places_nearby(
                GOOGLE_API_KEY, lat, lng, radius_m, type_="restaurant"
            )

        if competitors:
            comp_data = []
            for c in competitors:
                comp_data.append(
                    {
                        "Name": c.get("name"),
                        "Address": c.get("vicinity"),
                        "Rating": c.get("rating", None),
                        "Reviews": c.get("user_ratings_total", 0),
                    }
                )
            competitors_df = pd.DataFrame(comp_data)
            competitors_df = competitors_df[
                competitors_df["Name"].str.lower()
                != place_detail.get("name", "").lower()
            ]
            competitors_df = competitors_df.sort_values(
                by=["Rating", "Reviews"], ascending=[False, False]
            ).reset_index(drop=True)
            st.dataframe(competitors_df, use_container_width=True)
        else:
            st.info("æœªæ‰¾åˆ°ç«äº‰å¯¹æ‰‹ï¼ˆå¯èƒ½åŠå¾„å¤ªå°æˆ– API é™åˆ¶ï¼‰ã€‚")

    # ---- GBP è¯„åˆ† ----
    st.markdown("## 3ï¸âƒ£ Google å•†å®¶èµ„æ–™è¯„åˆ†ï¼ˆ40 åˆ†åˆ¶ï¼‰")
    gbp_result = score_gbp_profile(place_detail)
    st.metric("Google å•†å®¶èµ„æ–™å¾—åˆ†", f"{gbp_result['score']} / 40")

    gbp_rows = []
    for label, (pts, ok) in gbp_result["checks"].items():
        gbp_rows.append(
            {
                "æ£€æŸ¥é¡¹": label,
                "å¾—åˆ†": pts,
                "çŠ¶æ€": "âœ… å®Œæˆ" if ok else "âŒ ç¼ºå¤±/ä¸å®Œæ•´",
            }
        )
    st.table(pd.DataFrame(gbp_rows))

    # ---- ç½‘ç«™è¯„åˆ† ----
    st.markdown("## 4ï¸âƒ£ ç½‘ç«™å†…å®¹ & ä½“éªŒè¯„åˆ†ï¼ˆ40 åˆ†åˆ¶ï¼‰")
    effective_website = website_url or place_detail.get("website")

    if not effective_website:
        st.warning("æœªæä¾›ç½‘ç«™ URLï¼Œä¹Ÿæ— æ³•ä» Google è·å–ï¼Œç½‘ç«™è¯„åˆ†ä¸º 0ã€‚")
        web_result = {
            "score": 0,
            "checks": {"æ— ç½‘ç«™": (0, False)},
            "word_count": 0,
            "title": "",
            "text_snippet": "",
        }
    else:
        with st.spinner(f"æŠ“å–ç½‘ç«™ï¼š{effective_website}"):
            html = fetch_html(effective_website)
        web_result = score_website_basic(effective_website, html)

    st.metric("ç½‘ç«™å¾—åˆ†", f"{web_result['score']} / 40")
    web_rows = []
    for label, (pts, ok) in web_result["checks"].items():
        web_rows.append(
            {
                "æ£€æŸ¥é¡¹": label,
                "å¾—åˆ†": pts,
                "çŠ¶æ€": "âœ… æ˜¯" if ok else "âŒ å¦",
            }
        )
    st.table(pd.DataFrame(web_rows))

    # ---- å…³é”®è¯æ’å + è¥æ”¶æŸå¤± ----
    st.markdown("## 5ï¸âƒ£ æœ¬åœ°å…³é”®è¯æ’å & æ½œåœ¨è¥æ”¶æŸå¤±")

    keywords = [k.strip() for k in keywords_input.split(",") if k.strip()]
    rank_results: List[Dict[str, Any]] = []

    if not keywords:
        st.info("æœªæä¾›å…³é”®è¯ï¼Œè·³è¿‡æ’åæ¨¡æ‹Ÿå’Œè¥æ”¶ä¼°ç®—ã€‚")
    else:
        for kw in keywords:
            st.write(f"### å…³é”®è¯ï¼š**{kw}**")
            rank_bucket = "none"
            rank_position = None

            if SERPAPI_KEY and lat is not None and lng is not None:
                with st.spinner(f"ä½¿ç”¨ SerpAPI æŸ¥è¯¢ Google Maps æ’åï¼š{kw}"):
                    try:
                        serp_json = serpapi_google_maps_search(
                            SERPAPI_KEY, kw, lat, lng
                        )
                        rank_position = infer_rank_from_serpapi(
                            serp_json, place_detail.get("name", "")
                        )
                        if rank_position is not None:
                            if rank_position <= 3:
                                rank_bucket = "top3"
                            elif rank_position <= 10:
                                rank_bucket = "4-10"
                            else:
                                rank_bucket = "none"
                    except Exception as e:
                        st.warning(f"SerpAPI æŸ¥è¯¢å‡ºé”™ï¼š{e}")
                        rank_bucket = "none"
                        rank_position = None
            else:
                # æ²¡æœ‰ SerpAPIï¼šç”¨è¯„åˆ†+è¯„è®ºç®€å•è¿‘ä¼¼æ’åºï¼Œæ¨¡æ‹Ÿæœ¬åœ°æ’å
                if competitors:
                    all_places = competitors + [place_detail]
                    all_places_data = []
                    for p in all_places:
                        all_places_data.append(
                            {
                                "name": p.get("name", ""),
                                "rating": p.get("rating", 0),
                                "reviews": p.get("user_ratings_total", 0),
                            }
                        )
                    df_all = pd.DataFrame(all_places_data)
                    df_all["score"] = (
                        df_all["rating"].fillna(0) * 10
                        + df_all["reviews"].fillna(0) / 10
                    )
                    df_all = df_all.sort_values(
                        by="score", ascending=False
                    ).reset_index(drop=True)
                    positions = df_all["name"].str.lower().tolist()
                    name_lower = place_detail.get("name", "").lower()
                    if name_lower in positions:
                        pos = positions.index(name_lower) + 1
                        rank_position = pos
                        if pos <= 3:
                            rank_bucket = "top3"
                        elif pos <= 10:
                            rank_bucket = "4-10"
                        else:
                            rank_bucket = "none"

            monthly_loss = estimate_revenue_loss(
                monthly_search_volume,
                rank_bucket,
                avg_order_value,
                assumed_ctr,
                assumed_conv,
            )

            st.write(
                f"- ä¼°è®¡å½“å‰æ’åï¼š"
                f"{'Top 3' if rank_bucket=='top3' else ('ç¬¬ 4â€“10 å' if rank_bucket=='4-10' else 'æœªè¿›å…¥å‰ 10')}"
                f"{'' if rank_position is None else f'ï¼ˆæ¨æµ‹åæ¬¡ï¼š{rank_position}ï¼‰'}"
            )
            st.write(
                f"- ç²—ç•¥ä¼°è®¡ï¼šç”±äºæ²¡æœ‰åœ¨ç†æƒ³ä½ç½®ï¼Œ**æ¯æœˆå¯èƒ½å°‘èµšçº¦ ${monthly_loss:,.0f}**"
            )

            rank_results.append(
                {
                    "å…³é”®è¯": kw,
                    "é¢„ä¼°åæ¬¡": rank_position,
                    "åæ¬¡åŒºé—´": rank_bucket,
                    "é¢„ä¼°æœˆæŸå¤±($)": round(monthly_loss, 2),
                }
            )

        if rank_results:
            st.markdown("#### å…³é”®è¯ & è¥æ”¶æŸå¤±æ±‡æ€»")
            st.dataframe(pd.DataFrame(rank_results), use_container_width=True)

    # ---- ç»¼åˆå¾—åˆ† ----
    st.markdown("## 6ï¸âƒ£ æ€»ä½“åœ¨çº¿å¥åº·æ€»ç»“")
    total_score = gbp_result["score"] + web_result["score"]
    st.metric("ç»¼åˆå¾—åˆ†ï¼ˆProfile + Websiteï¼‰", f"{total_score} / 80")
    st.write(
        "- **40 åˆ†ä»¥ä¸‹**ï¼šåœ¨çº¿åŸºç¡€éå¸¸å¼±ï¼ŒåŸºæœ¬å±äº â€œPoorâ€ã€‚\n"
        "- **40â€“60 åˆ†**ï¼šä¸­ç­‰ï¼Œèƒ½è¢«æ‰¾å¾—åˆ°ï¼Œä½†ä¸å ä¼˜åŠ¿ã€‚\n"
        "- **60 åˆ†ä»¥ä¸Š**ï¼šç›¸å¯¹å¥åº·ï¼Œå¯ä»¥å¼€å§‹ç©ç²¾ç»†åŒ–è¿è¥å’Œæ´»åŠ¨ã€‚\n"
    )

    # =========================
    # ChatGPT æ·±åº¦å¤šç»´åˆ†æ
    # =========================
    st.markdown("## 7ï¸âƒ£ ChatGPT å¤šç»´èœç³» & è¿è¥åˆ†æ")

    if not OPENAI_API_KEY:
        st.warning("æœªé…ç½® OPENAI_API_KEYï¼Œå¦‚éœ€ AI æ·±åº¦åˆ†æè¯·åœ¨ Secrets ä¸­æ·»åŠ ã€‚")
    else:
        if st.button("ğŸ¤– ç”Ÿæˆ AI æ·±åº¦åˆ†æï¼ˆç»†åˆ†èœç³» + è¿è¥å»ºè®®ï¼‰"):
            with st.spinner("æ­£åœ¨è°ƒç”¨ ChatGPT åˆ†æï¼Œè¯·ç¨å€™..."):
                try:
                    ai_report = llm_deep_analysis(
                        place_detail=place_detail,
                        gbp_result=gbp_result,
                        web_result=web_result,
                        competitors_df=competitors_df,
                        rank_results=rank_results,
                        monthly_search_volume=monthly_search_volume,
                        avg_order_value=avg_order_value,
                    )
                    st.markdown(ai_report)
                except Exception as e:
                    st.error(f"è°ƒç”¨ ChatGPT API å‡ºé”™ï¼š{e}")
# ========== ç½²åï¼ˆLinkedInï¼‰ ==========
LINKEDIN_URL = "https://www.linkedin.com/in/lingyu-maxwell-lai"
st.markdown(
    f"""
<div style="display:flex;align-items:center;gap:10px;margin-top:-6px;margin-bottom:8px;">
  <div style="font-size:14px;color:#666;">
    Builded by <strong>Maxwell Lai</strong>
  </div>
  <a href="{LINKEDIN_URL}" target="_blank" title="LinkedIn: Maxwell Lai"
     style="display:inline-flex;align-items:center;justify-content:center;width:18px;height:18px;
            border-radius:4px;background:#0A66C2;">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/linkedin.svg"
         alt="LinkedIn" width="12" height="12" style="filter: invert(1);" />
  </a>
</div>
""",
    unsafe_allow_html=True,
)
