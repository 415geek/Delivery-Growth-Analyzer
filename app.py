import os
import json
import base64
from typing import List, Dict, Any, Optional

import streamlit as st
import pandas as pd
import requests
import googlemaps
from bs4 import BeautifulSoup
from urllib.parse import urlparse

from openai import OpenAI

# å°è¯•å¯é€‰å¯¼å…¥ headless æµè§ˆå™¨æ”¯æŒ
try:
    from requests_html import HTMLSession  # å¯èƒ½åœ¨æŸäº›ç¯å¢ƒç¼ºä¾èµ–
    HAS_REQUESTS_HTML = True
except Exception:
    HTMLSession = None
    HAS_REQUESTS_HTML = False

# =========================
# åŸºæœ¬é…ç½® & Secrets
# =========================
st.set_page_config(
    page_title="Aurainsight é¤é¦†å¢é•¿è¯Šæ–­",
    layout="wide",
)

st.title("Aurainsight é¤é¦†å¢é•¿è¯Šæ–­")
st.write(
    "é’ˆå¯¹åŒ—ç¾é¤é¦†è€æ¿çš„ä¸€é”®åœ¨çº¿ä½“æ£€ï¼š\n"
    "- åªéœ€è¾“å…¥åœ°å€ï¼Œè‡ªåŠ¨åŒ¹é…ä½ çš„é¤å…\n"
    "- è‡ªåŠ¨æ‰«æé™„è¿‘ç«äº‰å¯¹æ‰‹\n"
    "- ä¼°ç®—å ‚é£Ÿ / å¤–å–çš„æ½œåœ¨æµå¤±è¥æ”¶\n"
    "- æŠ“å–å®˜ç½‘ / å¤–å–å¹³å°èœå• + Google èœå•å›¾ç‰‡ï¼Œç»“åˆ ChatGPT åšå¤šç»´èœç³» & èœå•ç»“æ„ & è¿è¥åˆ†æ\n"
    "- åŸºäºèœå•èœç³»ç”»åƒï¼Œè‡ªåŠ¨ç­›é€‰çœŸæ­£çš„æ ¸å¿ƒç«å¯¹ï¼ˆå®éªŒåŠŸèƒ½ï¼‰"
)

# ä» Streamlit Secrets è¯»å– API å¯†é’¥
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY", "")
SERPAPI_KEY = st.secrets.get("SERPAPI_KEY", "")
YELP_API_KEY = st.secrets.get("YELP_API_KEY", "")
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")
SCRAPERAPI_KEY = st.secrets.get("SCRAPERAPI_KEY", "")

if not GOOGLE_API_KEY:
    st.error("ç¼ºå°‘ GOOGLE_API_KEYï¼Œè¯·å…ˆåœ¨ Streamlit Secrets ä¸­é…ç½®åå†åˆ·æ–°ã€‚")
    st.stop()

client: Optional[OpenAI] = None
if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)

# =========================
# Session State åˆå§‹åŒ–
# =========================
if "candidate_places" not in st.session_state:
    st.session_state["candidate_places"] = []
if "selected_index" not in st.session_state:
    st.session_state["selected_index"] = 0
if "analysis_ready" not in st.session_state:
    st.session_state["analysis_ready"] = False
if "ocr_menu_texts" not in st.session_state:
    st.session_state["ocr_menu_texts"] = []

# =========================
# å·¥å…·å‡½æ•°ï¼ˆå¸¦ç¼“å­˜ï¼‰
# =========================

@st.cache_data(show_spinner=False)
def gm_client(key: str):
    return googlemaps.Client(key=key)


@st.cache_data(show_spinner=False)
def google_geocode(api_key: str, address: str) -> List[Dict[str, Any]]:
    gmaps = gm_client(api_key)
    return gmaps.geocode(address)


@st.cache_data(show_spinner=False)
def google_place_details(api_key: str, place_id: str) -> Dict[str, Any]:
    """
    Google Place Detailsï¼š
    å…ˆå°è¯•å¸¦ fieldsï¼Œå¦‚æœ SDK/ç‰ˆæœ¬ä¸æ”¯æŒå°± fallback åˆ°ä¸å¸¦ fields çš„è°ƒç”¨ã€‚
    """
    gmaps = gm_client(api_key)
    fields = [
        "name",
        "formatted_address",
        "formatted_phone_number",
        "geometry",
        "rating",
        "user_ratings_total",
        "types",
        "opening_hours",
        "website",
        "price_level",
        "photos",
        "url",
    ]
    try:
        result = gmaps.place(place_id=place_id, fields=fields)
        data = result.get("result", result)
    except Exception:
        result = gmaps.place(place_id=place_id)
        data = result.get("result", result)
    return data


@st.cache_data(show_spinner=False)
def google_places_nearby(
    api_key: str, lat: float, lng: float, radius_m: int, type_: str = "restaurant"
) -> List[Dict[str, Any]]:
    gmaps = gm_client(api_key)
    result = gmaps.places_nearby(location=(lat, lng), radius=radius_m, type=type_)
    return result.get("results", [])


@st.cache_data(show_spinner=False)
def serpapi_google_maps_search(
    serpapi_key: str, query: str, lat: float, lng: float, zoom: float = 13.0
) -> Dict[str, Any]:
    url = "https://serpapi.com/search"
    ll_param = f"@{lat},{lng},{zoom}z"
    params = {
        "engine": "google_maps",
        "type": "search",
        "q": query,
        "ll": ll_param,
        "api_key": serpapi_key,
    }
    resp = requests.get(url, params=params, timeout=30)
    resp.raise_for_status()
    return resp.json()


# =========================
# ScraperAPI é›†æˆ
# =========================

@st.cache_data(show_spinner=False)
def fetch_html_via_scraperapi(url: str, render: bool = True) -> Optional[str]:
    """
    é€šè¿‡ ScraperAPI æŠ“å–é¡µé¢ï¼Œè‡ªåŠ¨ç»•è¿‡å¤§éƒ¨åˆ†åçˆ¬ & Cloudflareã€‚
    render=True ä¼šå¯ç”¨ JS æ¸²æŸ“ï¼Œé€‚åˆ order.online / Doordash è¿™ç±» SPAã€‚
    """
    if not SCRAPERAPI_KEY:
        return None

    api_endpoint = "https://api.scraperapi.com"
    params = {
        "api_key": SCRAPERAPI_KEY,
        "url": url,
    }
    if render:
        params["render"] = "true"

    try:
        resp = requests.get(api_endpoint, params=params, timeout=40)
        resp.raise_for_status()
        ctype = resp.headers.get("Content-Type", "")
        if "text/html" in ctype or "application/json" in ctype:
            return resp.text
        return None
    except Exception:
        return None


@st.cache_data(show_spinner=False)
def fetch_html(url: str) -> Optional[str]:
    """
    ç»Ÿä¸€é¡µé¢æŠ“å–é€»è¾‘ï¼š
    1ï¼‰é‡åˆ°å…¸å‹å¼º JS/åçˆ¬åŸŸåï¼ˆDoordash/order.online ç­‰ï¼‰ä¼˜å…ˆèµ° ScraperAPIï¼›
    2ï¼‰æ™®é€šè¯·æ±‚è¯•ä¸€æ¬¡ï¼›
    3ï¼‰å¤±è´¥å†èµ° ScraperAPIï¼›
    4ï¼‰å†å¤±è´¥ç”¨æœ¬åœ° headlessï¼ˆrequests_htmlï¼‰å…œåº•ã€‚
    """
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0 Safari/537.36"
        ),
        "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    }

    hard_domains = [
        "doordash.com",
        "ubereats.com",
        "grubhub.com",
        "order.online",
        "hungrypanda.co",
        "fantuan.ca",
        "chownow.com",
    ]
    lower_url = url.lower()

    # 0ï¸âƒ£ æŸäº›ç¬¬ä¸‰æ–¹ç‚¹é¤ç½‘ç«™ç›´æ¥èµ° ScraperAPI + JS æ¸²æŸ“
    if any(d in lower_url for d in hard_domains):
        html = fetch_html_via_scraperapi(url, render=True)
        if html:
            return html

    # 1ï¸âƒ£ æ™®é€šè¯·æ±‚ï¼ˆé€‚åˆè‡ªå®¶å®˜ç½‘ã€ç®€å•ç‚¹é¤ç«™ï¼‰
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        ctype = resp.headers.get("Content-Type", "")
        body = resp.text

        blocked = (
            resp.status_code >= 400
            or "captcha" in body.lower()
            or "access denied" in body.lower()
            or "temporarily blocked" in body.lower()
        )

        if resp.status_code < 400 and "text/html" in ctype and not blocked:
            return body
    except Exception:
        pass

    # 2ï¸âƒ£ æ™®é€šè¯·æ±‚å¤±è´¥ â†’ ScraperAPIï¼ˆæ¸²æŸ“æ‰“å¼€ï¼‰
    if SCRAPERAPI_KEY:
        html = fetch_html_via_scraperapi(url, render=True)
        if html:
            return html

    # 3ï¸âƒ£ å†å¤±è´¥ â†’ requests_html headless æ¸²æŸ“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if not HAS_REQUESTS_HTML:
        return None

    try:
        session = HTMLSession()
        r = session.get(url, headers=headers, timeout=30)
        r.html.render(timeout=40, sleep=2)
        return r.html.html
    except Exception:
        return None

# =========================
# Google èœå•ç…§ç‰‡ & OCR
# =========================

@st.cache_data(show_spinner=False)
def fetch_place_photo(api_key: str, photo_reference: str, maxwidth: int = 1200) -> bytes:
    """
    è°ƒç”¨ Google Place Photos APIï¼Œè¿”å›å›¾ç‰‡äºŒè¿›åˆ¶ã€‚
    """
    url = "https://maps.googleapis.com/maps/api/place/photo"
    params = {
        "key": api_key,
        "photoreference": photo_reference,
        "maxwidth": maxwidth,
    }
    resp = requests.get(url, params=params, timeout=30)
    resp.raise_for_status()
    return resp.content


def classify_menu_image(img_bytes: bytes) -> str:
    """
    ä½¿ç”¨ GPT å¤šæ¨¡æ€åˆ¤æ–­å›¾ç‰‡ç±»å‹ï¼š
    è¿”å›ï¼š
      - "menu_page"           æ˜æ˜¾æ˜¯èœç‰Œ/èœå•é¡µé¢
      - "food_dish"           å•é“èœ/å‡ é“èœçš„æ‘†ç›˜ç…§ç‰‡
      - "storefront_or_other" åº—æ‹›ã€Logoã€ç¯å¢ƒã€äººåƒç­‰
    """
    if client is None:
        return "storefront_or_other"

    b64 = base64.b64encode(img_bytes).decode("utf-8")
    data_url = f"data:image/jpeg;base64,{b64}"

    prompt = """
ä½ æ˜¯ä¸€åé¤é¥®å›¾ç‰‡è¯†åˆ«åŠ©æ‰‹ï¼Œè¯·åªæ ¹æ®å›¾ç‰‡å†…å®¹åˆ¤æ–­å›¾ç‰‡ç±»å‹ï¼Œä¸è¦åšå…¶ä»–äº‹æƒ…ã€‚

è¯·ä»ä¸‹é¢ä¸‰ç§ç±»å‹ä¸­é€‰ä¸€ä¸ªï¼Œå¹¶åªè¾“å‡ºå¯¹åº”çš„è‹±æ–‡ä»£ç ï¼ˆä¸è¦åŠ è§£é‡Šï¼‰ï¼š

1. å¦‚æœå›¾ç‰‡ä¸»è¦å†…å®¹æ˜¯ã€Œèœå•/èœç‰Œé¡µé¢ã€ï¼Œç‰¹å¾åŒ…æ‹¬ï¼š
   - æœ‰æˆåˆ—çš„èœå“åç§°ã€æè¿°å’Œä»·æ ¼
   - çœ‹èµ·æ¥åƒæ‰“å°å‡ºæ¥çš„ menu / laminated menu / æ‰‹å†™èœå•æ¿
   - å¯èƒ½æ˜¯ä¸€é¡µæˆ–å¤šé¡µèœå•çš„ç…§ç‰‡
   è¯·è¾“å‡ºï¼šmenu_page

2. å¦‚æœå›¾ç‰‡ä¸»è¦å†…å®¹æ˜¯ã€Œä¸€ç›˜æˆ–å‡ ç›˜èœã€é¥®å“ã€ï¼Œç‰¹å¾åŒ…æ‹¬ï¼š
   - çœ‹å¾—åˆ°å®é™…é£Ÿç‰©/é¥®æ–™æ‘†ç›˜
   - æ²¡æœ‰æˆåˆ—çš„èœå•æ¡ç›®å’Œä»·æ ¼
   è¯·è¾“å‡ºï¼šfood_dish

3. å¦‚æœå›¾ç‰‡ä¸»è¦å†…å®¹æ˜¯ã€Œåº—æ‹›ã€é—¨é¢ã€Logoã€ç¯å¢ƒã€äººåƒã€è¡—æ™¯ç­‰ã€ï¼Œè€Œä¸æ˜¯èœå•æˆ–èœå“ç‰¹å†™ï¼Œ
   è¯·è¾“å‡ºï¼šstorefront_or_other

é‡è¦è§„åˆ™ï¼š
- åªè¾“å‡ºä»¥ä¸Šä¸‰ç§ä¹‹ä¸€çš„è‹±æ–‡ä»£ç ï¼Œä¸è¦è¾“å‡ºä»»ä½•è¯´æ˜æ–‡å­—ã€‚
"""

    resp = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": data_url}},
                ],
            }
        ],
        temperature=0.0,
    )
    label = (resp.choices[0].message.content or "").strip().lower()
    if label not in {"menu_page", "food_dish", "storefront_or_other"}:
        return "storefront_or_other"
    return label


def get_place_photos(place_detail: Dict[str, Any], max_photos: int = 20) -> List[Dict[str, Any]]:
    """
    ä» Place Details ä¸­è·å–ç…§ç‰‡ï¼Œå¹¶è‡ªåŠ¨ç­›é€‰å‡ºâ€œèœå•é¡µâ€ä¼˜å…ˆè¿”å›ã€‚
    """
    photos = place_detail.get("photos", []) or []
    results: List[Dict[str, Any]] = []
    if not photos:
        return results

    for p in photos[:max_photos]:
        ref = p.get("photo_reference")
        if not ref:
            continue
        try:
            img_bytes = fetch_place_photo(GOOGLE_API_KEY, ref, maxwidth=1000)
        except Exception:
            continue

        label = classify_menu_image(img_bytes)
        if label == "menu_page":
            results.append(
                {
                    "photo_reference": ref,
                    "image_bytes": img_bytes,
                    "label": label,
                }
            )

    return results


def ocr_menu_from_image_bytes(img_bytes: bytes) -> str:
    """
    ä½¿ç”¨ OpenAI å¤šæ¨¡æ€ä»å›¾ç‰‡ä¸­æå–èœå•ä¿¡æ¯ï¼š
    - å¦‚æœæœ‰èœå•æ–‡å­—ï¼šè¾“å‡ºèœå + ä»·æ ¼
    - å¦‚æœæ— æ–‡å­—ä½†æœ‰èœå“ç…§ç‰‡ï¼šçŒœèœåï¼Œä»·æ ¼ unknown
    - å¦‚æœåªæ˜¯é—¨å¤´/ç¯å¢ƒ/Logoï¼šè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆå¿½ç•¥ï¼‰
    """
    if client is None:
        return ""

    b64 = base64.b64encode(img_bytes).decode("utf-8")
    data_url = f"data:image/jpeg;base64,{b64}"

    prompt = """
ä½ ç°åœ¨è¦åˆ¤æ–­è¿™å¼ å›¾ç‰‡æ˜¯ä¸æ˜¯â€œæœ‰ç”¨çš„èœå•ç›¸å…³å›¾ç‰‡â€ã€‚

è¯·æŒ‰ä»¥ä¸‹é€»è¾‘å¤„ç†ï¼š

1. å¦‚æœå›¾ç‰‡ä¸Šæœ‰æ˜æ˜¾çš„èœå•æ–‡å­—ï¼ˆä¾‹å¦‚èœåã€æè¿°ã€ä»·æ ¼ã€ç±»ä¼¼èœå•æ’ç‰ˆï¼‰ï¼š
   - åªæå–èœå“åç§°å’Œä»·æ ¼ã€‚
   - æ¯è¡Œè¾“å‡ºä¸€ä¸ªèœï¼Œæ ¼å¼ï¼š
     èœååŸæ–‡ - è‹±æ–‡å(å¦‚æœæœ‰å°±å†™ï¼Œæ²¡æœ‰å°±ç•™ç©º) - ä»·æ ¼
   - å¦‚æœæ˜¯å¤šç§è§„æ ¼ï¼Œå¯ä»¥æ‹†æˆå¤šè¡Œã€‚
   - ä¸è¦è¾“å‡ºä»»ä½•é¢å¤–è¯´æ˜ã€‚

2. å¦‚æœå›¾ç‰‡ä¸Šæ²¡æœ‰æ˜æ˜¾çš„æ–‡å­—ï¼Œä½†èƒ½æ¸…æ¥šçœ‹åˆ°ä¸€ç›˜èœæˆ–ä¸€æ¯é¥®æ–™ç­‰â€œå•å“èœå“ç…§ç‰‡â€ï¼š
   - çŒœæµ‹è¯¥èœ/é¥®å“æœ€å¯èƒ½çš„ä¸­è‹±æ–‡åç§°ã€‚
   - æ¯è¡Œè¾“å‡ºä¸€ä¸ªå€™é€‰ï¼Œæ ¼å¼ï¼š
     çŒœæµ‹èœå(ä¸­æ–‡ï¼Œå¦‚æœä½ çŸ¥é“) - English name(å¦‚æœèƒ½åˆ¤æ–­) - unknown
   - æœ€å¤šè¾“å‡º 1-3 è¡Œã€‚
   - ä¸è¦è¾“å‡ºå…¶ä»–è§£é‡Šã€‚

3. å¦‚æœå›¾ç‰‡ä¸»è¦æ˜¯åº—æ‹›ã€Logoã€äººåƒã€è¡—æ™¯ã€å®¤å†…ç¯å¢ƒï¼Œæ²¡æœ‰å¯è¯†åˆ«çš„èœå•æ–‡å­—ï¼Œä¹Ÿçœ‹ä¸æ¸…å…·ä½“èœå“ï¼š
   - ä¸è¦è¾“å‡ºä»»ä½•å†…å®¹ï¼Œè¿”å›å®Œå…¨ç©ºçš„ç»“æœã€‚

æ€»è§„åˆ™ï¼š
- åªè¾“å‡ºèœå•æ¡ç›®æ–‡æœ¬ï¼Œä¸è¦åŠ æ ‡é¢˜ã€è¯´æ˜ã€å‰åç¼€ã€‚
- å¦‚æœæœ€ååˆ¤æ–­å±äºç¬¬ 3 ç§æƒ…å†µï¼Œå°±è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
"""

    resp = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": data_url}},
                ],
            }
        ],
        temperature=0.1,
    )
    text = resp.choices[0].message.content or ""
    return text.strip()

# =========================
# è¯„åˆ† & è®¡ç®—å‡½æ•°
# =========================

def score_gbp_profile(place: Dict[str, Any]) -> Dict[str, Any]:
    """ç®€åŒ–ç‰ˆ Google å•†å®¶èµ„æ–™è¯„åˆ†ï¼Œæ€»åˆ† 40 åˆ†ã€‚"""
    score = 0
    checks: Dict[str, Any] = {}

    has_name = bool(place.get("name"))
    has_address = bool(place.get("formatted_address"))
    pts = 4 if (has_name and has_address) else 0
    score += pts
    checks["åç§°/åœ°å€å®Œæ•´"] = (pts, has_name and has_address)

    has_phone = bool(place.get("formatted_phone_number"))
    pts = 4 if has_phone else 0
    score += pts
    checks["ç”µè¯"] = (pts, has_phone)

    opening_hours = place.get("opening_hours", {})
    has_hours = bool(opening_hours.get("weekday_text")) or opening_hours.get("open_now") is not None
    pts = 4 if has_hours else 0
    score += pts
    checks["è¥ä¸šæ—¶é—´"] = (pts, has_hours)

    has_website = bool(place.get("website"))
    pts = 4 if has_website else 0
    score += pts
    checks["ç½‘ç«™é“¾æ¥"] = (pts, has_website)

    rating = place.get("rating")
    reviews = place.get("user_ratings_total", 0)
    has_reviews = rating is not None and reviews >= 10
    pts = 6 if has_reviews else 0
    score += pts
    checks["è¯„åˆ† & â‰¥10æ¡è¯„è®º"] = (pts, has_reviews)

    types_ = place.get("types", [])
    has_category = any(t for t in types_ if t != "point_of_interest")
    pts = 6 if has_category else 0
    score += pts
    checks["ç±»åˆ«è®¾ç½®"] = (pts, has_category)

    has_price_level = place.get("price_level") is not None
    pts = 4 if has_price_level else 0
    score += pts
    checks["ä»·æ ¼åŒºé—´"] = (pts, has_price_level)

    photos = place.get("photos", [])
    has_photos = len(photos) > 0
    pts = 8 if has_photos else 0
    score += pts
    checks["ç…§ç‰‡/å›¾ç‰‡"] = (pts, has_photos)

    return {"score": score, "checks": checks}


def score_website_basic(url: str, html: Optional[str]) -> Dict[str, Any]:
    """ç®€åŒ–ç‰ˆç½‘ç«™è¯„åˆ†ï¼Œæ€»åˆ† 40 åˆ† + è¿”å›æ–‡æœ¬æ‘˜è¦ã€‚"""
    if not url or not html:
        return {
            "score": 0,
            "checks": {"æ— æ³•è®¿é—®ç½‘ç«™": (0, False)},
            "word_count": 0,
            "title": "",
            "text_snippet": "",
        }

    soup = BeautifulSoup(html, "lxml")
    score = 0
    checks: Dict[str, Any] = {}

    texts = soup.get_text(separator=" ", strip=True)
    word_count = len(texts.split())
    text_snippet = texts[:3000]

    title = soup.title.string.strip() if soup.title and soup.title.string else ""
    has_title = bool(title)
    pts = 6 if has_title else 0
    score += pts
    checks["æœ‰é¡µé¢æ ‡é¢˜ï¼ˆtitleï¼‰"] = (pts, has_title)

    desc_tag = soup.find("meta", attrs={"name": "description"})
    has_desc = bool(desc_tag and desc_tag.get("content"))
    pts = 6 if has_desc else 0
    score += pts
    checks["æœ‰ Meta Description"] = (pts, has_desc)

    h1 = soup.find("h1")
    has_h1 = bool(h1 and h1.get_text(strip=True))
    pts = 4 if has_h1 else 0
    score += pts
    checks["æœ‰ H1 æ ‡é¢˜"] = (pts, has_h1)

    has_sufficient_text = word_count >= 300
    pts = 8 if has_sufficient_text else 0
    score += pts
    checks["æ–‡æœ¬é‡ â‰¥ 300 è¯"] = (pts, has_sufficient_text)

    has_phone_text = any(x in texts for x in ["(", ")", "-", "+1"])
    pts = 4 if has_phone_text else 0
    score += pts
    checks["é¡µé¢ä¸Šèƒ½çœ‹åˆ°ç”µè¯"] = (pts, has_phone_text)

    keywords = [
        "chinese", "cantonese", "szechuan", "sichuan", "shanghai",
        "dim sum", "noodle", "rice", "dumpling", "hot pot", "bbq"
    ]
    kw_hit = any(kw.lower() in texts.lower() for kw in keywords)
    pts = 6 if kw_hit else 0
    score += pts
    checks["æ–‡æœ¬åŒ…å«èœå“/èœç³»å…³é”®è¯"] = (pts, kw_hit)

    parsed = urlparse(url)
    has_https = parsed.scheme == "https"
    pts = 6 if has_https else 0
    score += pts
    checks["ä½¿ç”¨ HTTPS"] = (pts, has_https)

    return {
        "score": score,
        "checks": checks,
        "word_count": word_count,
        "title": title,
        "text_snippet": text_snippet,
    }


def estimate_revenue_loss(
    monthly_search_volume: int,
    rank_bucket: str,
    avg_order_value: float,
    channel: str = "dine-in",
) -> float:
    """ç²—ç•¥è¥æ”¶æŸå¤±ä¼°ç®—ï¼ˆå†…éƒ¨ CTR/è½¬åŒ–ç‡å‡è®¾ï¼‰ã€‚"""
    if channel == "delivery":
        ctr = 0.18
        conv = 0.35
    else:
        ctr = 0.12
        conv = 0.25

    ideal_customers = monthly_search_volume * ctr * conv
    if rank_bucket == "top3":
        current_factor = 1.0
    elif rank_bucket == "4-10":
        current_factor = 0.4
    elif rank_bucket == "11+":
        current_factor = 0.1
    else:  # none / unknown
        current_factor = 0.0

    current_customers = ideal_customers * current_factor
    potential_extra_customers = ideal_customers - current_customers
    monthly_loss = potential_extra_customers * avg_order_value
    return monthly_loss


def infer_rank_from_serpapi(
    serp_json: Dict[str, Any], business_name: str
) -> Optional[int]:
    """ä» SerpAPI Google Maps ç»“æœä¸­æ‰¾åˆ°å½“å‰é¤å…åæ¬¡ã€‚"""
    results = serp_json.get("local_results") or serp_json.get("places_results") or []
    for idx, res in enumerate(results, start=1):
        name = res.get("title") or res.get("name", "")
        if name and business_name.lower() in name.lower():
            return idx
    return None

# =========================
# èœå•ç›¸å…³ & èœç³»ç”»åƒ
# =========================

def extract_menu_text_from_html(html: str) -> str:
    """ä» HTML ä¸­å°½é‡æå–å‡ºåƒèœå•çš„å†…å®¹ï¼ˆèœå + ä»·æ ¼ç­‰ï¼‰"""
    soup = BeautifulSoup(html, "lxml")

    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    texts = []
    for el in soup.find_all(["h2", "h3", "h4", "li", "p", "span", "div"]):
        txt = el.get_text(" ", strip=True)
        if not txt:
            continue
        if any(x in txt for x in ["$", "Â¥"]) or any(
            kw in txt.lower()
            for kw in ["chicken", "beef", "pork", "noodle", "rice", "tofu", "dumpling", "soup"]
        ):
            if 3 <= len(txt) <= 120:
                texts.append(txt)

    if not texts:
        full = soup.get_text(" ", strip=True)
        return full[:4000]

    seen = set()
    deduped = []
    for t in texts:
        if t not in seen:
            seen.add(t)
            deduped.append(t)

    return "\n".join(deduped[:400])


def build_menu_payload(menu_urls: List[str]) -> List[Dict[str, str]]:
    menus: List[Dict[str, str]] = []
    for url in menu_urls:
        url = url.strip()
        if not url:
            continue

        html = fetch_html(url)
        if not html:
            menus.append(
                {
                    "source": urlparse(url).netloc or "unknown",
                    "url": url,
                    "status": "fetch_failed_or_blocked",
                    "menu_text": "",
                }
            )
            continue

        menu_text = extract_menu_text_from_html(html)
        status = "ok" if menu_text.strip() else "no_menu_detected"

        menus.append(
            {
                "source": urlparse(url).netloc or "unknown",
                "url": url,
                "status": status,
                "menu_text": menu_text,
            }
        )

    return menus


def discover_menu_urls(place_detail: Dict[str, Any], website_html: Optional[str]) -> List[str]:
    """
    å°è¯•è‡ªåŠ¨å‘ç°èœå•/ç‚¹é¤é“¾æ¥ï¼š
    - è‡ªå®¶å®˜ç½‘
    - å®˜ç½‘é¡µé¢é‡ŒåŒ…å« menu/order çš„é“¾æ¥
    - å¸¸è§ç¬¬ä¸‰æ–¹å¤–å–å¹³å°é“¾æ¥
    """
    urls = set()

    main_site = place_detail.get("website")
    if main_site:
        urls.add(main_site)

    if "url" in place_detail:
        urls.add(place_detail["url"])

    if website_html:
        soup = BeautifulSoup(website_html, "lxml")
        for a in soup.find_all("a", href=True):
            href = a["href"]
            href_lower = href.lower()
            text = a.get_text(" ", strip=True).lower()

            if any(k in href_lower for k in ["menu", "order", "online-order", "order-online"]) or \
               any(k in text for k in ["menu", "order", "online order"]):
                urls.add(href)

            for domain in [
                "doordash.com",
                "ubereats.com",
                "grubhub.com",
                "hungrypanda.co",
                "fantuan.ca",
                "order.online",
                "chownow.com",
            ]:
                if domain in href_lower:
                    urls.add(href)

    return list(urls)

# ========== èœå•èœç³»ç”»åƒ & ç²¾å‡†ç«å¯¹è¾…åŠ©å‡½æ•° ==========

def analyze_menu_profile(menu_text: str) -> Dict[str, Any]:
    """
    ç”¨ ChatGPT æ ¹æ®èœå•æ–‡æœ¬åšèœç³»ç”»åƒï¼ˆå·èœ / ç²¤èœ / æ¸¯å¼èŒ¶é¤å… / ç‚¹å¿ƒ / å¥¶èŒ¶åº—ç­‰ï¼‰
    """
    if client is None:
        return {"error": "æœªé…ç½® OPENAI_API_KEYï¼Œæ— æ³•è¿›è¡Œèœç³»ç”»åƒåˆ†æã€‚"}

    system_prompt = """
ä½ æ˜¯ä¸€åç†Ÿæ‚‰åŒ—ç¾ä¸­é¤å¸‚åœºçš„é¤é¥®é¡¾é—®ï¼Œä¸“é—¨æ ¹æ®èœå•å†…å®¹ç»™é¤å…åšç”»åƒã€‚

ç‰¹åˆ«è§„åˆ™ï¼ˆå¾ˆé‡è¦ï¼‰ï¼š
- å¦‚æœèœå•é‡Œå‡ºç°å¤§é‡â€œç„—é¥­ã€ç„—çŒªæ‰’é¥­ã€æ„ç²‰ï¼ˆæ„å¤§åˆ©é¢ï¼‰ã€å…¬ä»”é¢ã€è èæ²¹ã€å¤šå£«ã€ä¸‰æ–‡æ²»â€ç­‰ï¼Œ
  å¹¶ä¸”åŒæ—¶æœ‰æ¸¯å¼å¥¶èŒ¶ã€é¸³é¸¯ç­‰é¥®å“ï¼Œè¿™å®¶åº—å¾ˆå¤§æ¦‚ç‡æ˜¯ã€æ¸¯å¼èŒ¶é¤å…ã€‘ã€‚
- å¦‚æœèœå•é‡Œå‡ºç°å¤§é‡â€œè’¸æ’éª¨ã€å‡¤çˆªã€èåœç³•ã€è™¾é¥ºã€çƒ§å–ã€è‚ ç²‰ã€å‰çƒ§åŒ…ã€æµæ²™åŒ…â€ç­‰ç‚¹å¿ƒç±»èœå“ï¼Œ
  å¹¶ä»¥ä¸€ç¬¼ä¸€ç¬¼çš„å°ä»½ä¸ºä¸»ï¼Œè¿™å®¶åº—å¾ˆå¤§æ¦‚ç‡æ˜¯ã€ç²¤å¼æ—©èŒ¶/ç‚¹å¿ƒä¸ºä¸»çš„ç²¤èœé¦†ã€‘ã€‚
- å¦‚æœä¸¤è€…éƒ½æœ‰ï¼Œè¦çœ‹å“ªä¸€ç±»èœå“å æ¯”æ›´é«˜ï¼š
  - èŒ¶é¤å…ï¼šä¸»é£Ÿç±»ç„—é¥­/æ„ç²‰/å…¬ä»”é¢/å¥—é¤å¤šï¼Œç‚¹å¿ƒåªæ˜¯å°‘é‡è¡¥å……ã€‚
  - ç²¤èœé…’æ¥¼/ç‚¹å¿ƒåº—ï¼šç‚¹å¿ƒç±»å“ç§éå¸¸å¤šï¼Œç„—é¥­/æ„ç²‰åªæ˜¯å°‘é‡å‡ºç°ã€‚

- å·èœç‰¹å¾å…³é”®è¯ä¸¾ä¾‹ï¼šæ°´ç…®é±¼ã€éº»å©†è±†è…ã€æ¯›è¡€æ—ºã€é…¸èœé±¼ã€è¾£å­é¸¡ã€å¹²é”…ã€å†’èœã€ä¸²ä¸²é¦™ç­‰ã€‚
- æ¹˜èœç‰¹å¾å…³é”®è¯ä¸¾ä¾‹ï¼šå‰æ¤’é±¼å¤´ã€å†œå®¶å°ç‚’è‚‰ã€æ‰‹æ’•åŒ…èœã€è‡­è±†è…ã€å£å‘³è™¾ç­‰ã€‚
- åŒ—æ–¹é¢é¦†/é¥ºå­é¦†å¯ä»¥åŒ…å«ï¼šé¥ºå­ã€é”…è´´ã€æ‰‹å·¥é¢ã€ç‰›è‚‰é¢ã€ç¾Šè‚‰ä¸²ã€é”…åŒ…è‚‰ç­‰ã€‚

è¾“å‡ºå¿…é¡»æ˜¯ JSONï¼Œå­—æ®µå¦‚ä¸‹ï¼š
- primary_cuisine: ä¸»èœç³»ï¼Œæ¯”å¦‚ "å·èœ", "ç²¤èœ", "æ¸¯å¼èŒ¶é¤å…", "ç²¤å¼ç‚¹å¿ƒ", "é¢åŒ…åº—", "å¥¶èŒ¶åº—", "å…¶ä»–ä¸­é¤"
- secondary_cuisines: å¯èƒ½çš„æ¬¡è¦èœç³»åˆ—è¡¨ï¼Œæ¯”å¦‚ ["ç²¤èœ", "æ¸¯å¼èŒ¶é¤å…"]
- business_type: "æ­£é¤" / "å¿«é¤" / "æ‰‹æ‘‡é¥®" / "çƒ˜ç„™ç”œå“"
- price_level: ä» 1 åˆ° 4, å¯¹åº”äººå‡å¤§æ¦‚ $: 1=ä¾¿å®œ, 2=ä¸­ç­‰, 3=åé«˜, 4=é«˜ç«¯
- signature_items: èœå•ä¸­ä½ è®¤ä¸ºæœ€èƒ½ä»£è¡¨è¿™å®¶åº—é£æ ¼çš„ 3-5 ä¸ªèœå“åï¼ˆç”¨åŸæ–‡ï¼‰
- competitor_search_keywords: æœç´¢ç«å¯¹æ—¶å»ºè®®ç”¨çš„è‹±æ–‡å…³é”®è¯åˆ—è¡¨
- notes: ä½ çš„åˆ¤æ–­ä¾æ®å’Œæé†’ï¼ˆä¸­æ–‡ï¼‰
åªè¾“å‡º JSONã€‚
    """.strip()

    user_prompt = f"ä»¥ä¸‹æ˜¯è¿™å®¶é¤å…çš„èœå•å†…å®¹ï¼ˆèœå+ç®€ä»‹ï¼Œå¯ä»¥ä¸å®Œæ•´ï¼‰ï¼š\n\n{menu_text}\n\nè¯·æ ¹æ®ä¸Šé¢çš„è¦æ±‚è¾“å‡º JSONã€‚"

    resp = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        response_format={"type": "json_object"},
        temperature=0.2,
    )
    return json.loads(resp.choices[0].message.content)


def build_competitor_profiles(
    competitors_df: pd.DataFrame,
    api_key: str,
    max_n: int = 15,
) -> List[Dict[str, Any]]:
    """
    å°†é™„è¿‘ç«äº‰å¯¹æ‰‹çš„åŸºç¡€ä¿¡æ¯ + Google è¯¦æƒ…æ•´ç†æˆç»™ AI ç”¨çš„ç®€æ´ç»“æ„ã€‚
    ä¸ºæ§åˆ¶è°ƒç”¨æ¬¡æ•°ï¼Œåªå–è¯„åˆ†é å‰çš„å‰ max_n å®¶ã€‚
    """
    profiles: List[Dict[str, Any]] = []
    if competitors_df is None or competitors_df.empty:
        return profiles

    subset = competitors_df.head(max_n)
    for _, row in subset.iterrows():
        pid = row.get("place_id")
        if not pid:
            continue
        try:
            detail = google_place_details(api_key, pid)
        except Exception:
            detail = {}

        profiles.append(
            {
                "name": detail.get("name") or row.get("name"),
                "vicinity": detail.get("formatted_address") or row.get("vicinity"),
                "rating": detail.get("rating") or row.get("rating"),
                "reviews": detail.get("user_ratings_total") or row.get("reviews"),
                "price_level": detail.get("price_level"),
                "types": detail.get("types", []),
            }
        )
    return profiles


def rank_competitors_with_gpt(
    profile: Dict[str, Any],
    candidates: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    è®© ChatGPT åœ¨å€™é€‰é¤å…ä¸­æŒ‘å‡ºçœŸæ­£çš„ 5â€“10 å®¶æ ¸å¿ƒç«å¯¹ï¼Œå¹¶æŒ‰ç›¸ä¼¼åº¦æ’åºã€‚
    """
    if client is None:
        return []

    system_prompt = """
ä½ æ˜¯ä¸€åç†Ÿæ‚‰åŒ—ç¾ä¸­é¤å¸‚åœºçš„ç«å¯¹åˆ†æå¸ˆã€‚
ç°åœ¨æœ‰ä¸€é—´ç›®æ ‡é¤å…çš„èœç³»ç”»åƒï¼Œä»¥åŠä¸€æ‰¹é™„è¿‘å€™é€‰é¤å…çš„ä¿¡æ¯ã€‚
è¯·ä½ ä»å€™é€‰ä¸­é€‰å‡ºæœ€åƒçš„ 5-10 å®¶ç«å¯¹ï¼Œå¹¶æŒ‰ç›¸ä¼¼åº¦ä»é«˜åˆ°ä½æ’åºã€‚

ç›¸ä¼¼åº¦åˆ¤æ–­ç»´åº¦åŒ…æ‹¬ï¼š
- èœç³» / ç±»åˆ«æ˜¯å¦æ¥è¿‘ï¼ˆæ¯”å¦‚éƒ½æ˜¯å·èœã€ç²¤èœã€æ¸¯å¼èŒ¶é¤å…ç­‰ï¼‰
- ä»·æ ¼å¸¦æ˜¯å¦æ¥è¿‘
- æ˜¯å¦å±äºç›¸ä¼¼ä¸šæ€ï¼ˆæ­£é¤/å¿«é¤/èŒ¶é¤å…/å¥¶èŒ¶åº—/çƒ˜ç„™åº—ï¼‰
- è‹¥ä¿¡æ¯æœ‰é™ï¼Œå¯æ ¹æ®åˆ†ç±» types å’Œé¤å…åç§°åšåˆç†æ¨æ–­

è¾“å‡º JSON å¯¹è±¡ï¼š
{
  "competitors": [
    {
      "name": "...",
      "similarity_score": 0-100,
      "main_reason": "1-2 å¥ä¸­æ–‡è§£é‡Š",
      "vicinity": "...",
      "rating": 4.5,
      "reviews": 123,
      "price_level": 2,
      "types": ["chinese", "restaurant"]
    },
    ...
  ]
}
    """.strip()

    user_content = {
        "target_profile": profile,
        "candidates": candidates,
    }

    resp = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": json.dumps(user_content, ensure_ascii=False)},
        ],
        response_format={"type": "json_object"},
        temperature=0.3,
    )

    data = json.loads(resp.choices[0].message.content)
    return data.get("competitors", [])

# =========================
# ChatGPT æ·±åº¦åˆ†æå‡½æ•°
# =========================

def call_llm_safe(messages: List[Dict[str, Any]]) -> str:
    if client is None:
        return "æœªé…ç½® OPENAI_API_KEYï¼Œæ— æ³•è°ƒç”¨ ChatGPTï¼Œè¯·åœ¨ Streamlit Secrets ä¸­æ·»åŠ  OPENAI_API_KEYã€‚"
    try:
        completion = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=messages,
            temperature=0.4,
        )
        return completion.choices[0].message.content
    except Exception as e:
        try:
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.4,
            )
            return completion.choices[0].message.content
        except Exception as e2:
            return f"è°ƒç”¨ ChatGPT å¤±è´¥ã€‚\nä¸»æ¨¡å‹é”™è¯¯ï¼š{e}\nå¤‡ç”¨æ¨¡å‹é”™è¯¯ï¼š{e2}"


def llm_deep_analysis(
    place_detail: Dict[str, Any],
    gbp_result: Dict[str, Any],
    web_result: Dict[str, Any],
    competitors_df: Optional[pd.DataFrame],
    rank_results: List[Dict[str, Any]],
    monthly_search_volume: int,
    dine_in_aov: float,
    delivery_aov: float,
    menus_payload: List[Dict[str, str]],
) -> str:
    comp_json = []
    if competitors_df is not None and not competitors_df.empty:
        sub = competitors_df.head(6)
        comp_json = sub.to_dict(orient="records")

    payload = {
        "restaurant": {
            "name": place_detail.get("name"),
            "address": place_detail.get("formatted_address"),
            "phone": place_detail.get("formatted_phone_number"),
            "types": place_detail.get("types", []),
            "rating": place_detail.get("rating"),
            "reviews": place_detail.get("user_ratings_total"),
            "price_level": place_detail.get("price_level"),
        },
        "gbp_score": gbp_result["score"],
        "gbp_checks": gbp_result["checks"],
        "website_score": web_result["score"],
        "website_title": web_result.get("title", ""),
        "website_word_count": web_result.get("word_count", 0),
        "competitors": comp_json,
        "rank_results": rank_results,
        "assumptions": {
            "monthly_search_volume_per_keyword": monthly_search_volume,
            "dine_in_aov": dine_in_aov,
            "delivery_aov": delivery_aov,
        },
        "menus": menus_payload,
    }

    text_snippet = web_result.get("text_snippet", "")

    system_msg = (
        "ä½ æ˜¯ä¸€åä¸“é—¨æœåŠ¡åŒ—ç¾é¤é¦†çš„æœ¬åœ°è¥é”€å’Œå¤–å–è¿è¥é¡¾é—®ï¼Œæ›¾ä»»èŒäºéº¦è‚¯é”¡ä¸€ä¸ªä¸“é—¨åšé¤é¥®åˆ†æçš„éƒ¨é—¨ï¼Œ"
        "éå¸¸äº†è§£ä¸–ç•Œå„åœ°çš„èœç³»ï¼Œå°¤å…¶åœ¨ä¸­é¤èœç³»çš„ç»†åˆ†é¢†åŸŸå±äºè¡Œä¸šæƒå¨ï¼Œå¦‚ç²¤èœã€èŒ¶é¤å…ã€å·èœã€æ¹˜èœã€ä¸œåŒ—èœã€ä¸Šæµ·èœç­‰ï¼Œ"
        "ç†Ÿæ‚‰ Google æœ¬åœ°æœç´¢å’Œ UberEats/DoorDash/Grubhub/Hungrypanda/Fantuan ç­‰å¹³å°çš„è¿è¥é€»è¾‘ã€‚"
        "è¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ï¼Œä½†åœ¨éœ€è¦æ—¶å¯åŠ å°‘é‡è‹±æ–‡æœ¯è¯­ã€‚"
    )

    user_msg = f"""
è¿™æ˜¯ä¸€ä¸ªé¤å…çš„åœ¨çº¿æ•°æ®å’Œèœå•ç‰‡æ®µï¼Œè¯·ä½ åš**å¤šç»´æ·±åº¦åˆ†æ**ï¼š

ã€ç»“æ„åŒ–æ•°æ® JSONã€‘
{json.dumps(payload, ensure_ascii=False, indent=2)}

ã€ç½‘ç«™æ–‡æœ¬ç‰‡æ®µï¼ˆæœ€å¤š 3000 å­—ç¬¦ï¼‰ã€‘
{text_snippet}

è¯·ä½ å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼ˆåˆ†æ®µè¾“å‡ºï¼‰ï¼š

1. **èœç³»ç»†åˆ†åˆ¤æ–­**
   - åˆ¤æ–­è¯¥åº—çš„ä¸»èœç³»å’Œå­èœç³»ï¼ˆä¾‹å¦‚ï¼šç²¤èœ-èŒ¶é¤å…ã€å·èœ-è¾£ç‚’ã€ä¸œåŒ—å®¶å¸¸èœã€ä¸Šæµ·èœç­‰ï¼‰ï¼Œè¯´æ˜ä¾æ®ã€‚
   - å¦‚æœèœå•é‡Œæœ‰å¤šç§èœç³»ï¼Œè¯·è¯´æ˜ä¸»æ¬¡ç»“æ„ã€‚

2. **èœå•ç»“æ„ä¸ä»·æ ¼å¸¦åˆ†æ**
   - æ ¹æ®èœå•æ–‡æœ¬ï¼Œåˆ†æï¼š
     - çƒ­é—¨å“ç±»ï¼ˆå¦‚ä¸»é£Ÿç±»ã€æ‹›ç‰Œèœã€å¥—é¤ã€ç‚¸é¸¡ã€ç”œå“ç­‰ï¼‰
     - äººå‡ä»·ä½åŒºé—´ã€ä¸»åŠ›ä»·æ ¼å¸¦ï¼ˆä¾‹å¦‚ï¼šå¤šæ•°ä¸»èœé›†ä¸­åœ¨ $15â€“$22ï¼‰
     - æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„â€œåˆ©æ¶¦æ€æ‰‹â€ï¼ˆä»·æ ¼åä½ä½†åˆ¶ä½œå¤æ‚ã€æ¯›åˆ©ä½çš„èœï¼‰ã€‚

3. **çº¿ä¸Šæ›å…‰ & ç«äº‰æ€åŠ¿è§£è¯»**
   - ç»“åˆ GBP è¯„åˆ†ã€ç½‘ç«™å¾—åˆ†ã€å…³é”®è¯æ’åç»“æœï¼Œåˆ¤æ–­ï¼š
     - ç›®å‰åœ¨æœ¬åœ°æœç´¢ä¸­çš„ä½ç½®ï¼ˆè½åç¨‹åº¦ã€æœ‰æ— æœºä¼šå†²å‡» Top 3ï¼‰ã€‚
     - å’Œ 3â€“5 å®¶æ ¸å¿ƒç«å“ç›¸æ¯”çš„æ˜æ˜¾çŸ­æ¿å’Œä¼˜åŠ¿ã€‚

4. **å¤–å–å¹³å°æœºä¼šç‚¹ï¼ˆå¦‚æœèœå•é‡Œå‡ºç°å¤–å–å¹³å°é“¾æ¥ï¼‰**
   - æ ¹æ®èœå“ç»“æ„å’Œä»·æ ¼ï¼Œåˆ¤æ–­é€‚åˆé‡ç‚¹å‘åŠ›çš„å¹³å°ç±»å‹ï¼ˆèšåˆå¤–å– / è‡ªé…é€ / çº¿ä¸‹å ‚é£Ÿå¼•æµï¼‰ã€‚
   - ç»™å‡º 2â€“3 ä¸ªå…·ä½“å¯æ‰§è¡Œçš„ä¿ƒé”€æ´»åŠ¨å»ºè®®ï¼ˆæ¯”å¦‚ï¼šé«˜æ¯›åˆ©å“ç±»åš BOGOã€åˆå¸‚å®šä»·é€»è¾‘ç­‰ï¼‰ã€‚

5. **æ¥ä¸‹æ¥ 30 å¤©å¯æ‰§è¡Œçš„è¡ŒåŠ¨æ¸…å•**
   - ç”¨æ¸…å•æ–¹å¼ç»™å‡º 5â€“8 æ¡â€œé¤é¦†è€æ¿èƒ½å¬æ‡‚ã€èƒ½é©¬ä¸Šæ‰§è¡Œâ€çš„æ”¹è¿›å»ºè®®ï¼š
     - Google èµ„æ–™ & ç½‘ç«™å†…å®¹ä¼˜å…ˆçº§ï¼›
     - èœå•ç»“æ„å’Œå®šä»·ä¼˜åŒ–ï¼›
     - å¤–å–æ´»åŠ¨ & è½¬åŒ–ç‡ä¼˜åŒ–å»ºè®®ã€‚

è¦æ±‚ï¼š
- å°½é‡ç”¨çŸ­å¥å’Œé¡¹ç›®ç¬¦å·ï¼Œæ–¹ä¾¿é¤å…è€æ¿é˜…è¯»å’Œæ‰§è¡Œã€‚
- å¯¹æ¯æ¡å»ºè®®ï¼Œç®€å•è¯´æ˜â€œä¸ºä»€ä¹ˆè¿™ä¹ˆåšæœ‰ç”¨â€ï¼ˆåŸºäºæ•°æ®/ç»éªŒçš„é€»è¾‘ï¼‰ã€‚
"""

    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ]
    return call_llm_safe(messages)

# =========================
# 1ï¸âƒ£ è¾“å…¥åœ°å€ï¼Œé”å®šé¤å…
# =========================

st.markdown("## 1ï¸âƒ£ è¾“å…¥é¤å…åœ°å€ï¼ˆè‡ªåŠ¨åŒ¹é…é™„è¿‘é¤å…ï¼‰")

address_input = st.text_input(
    "é¤å…åœ°å€ï¼ˆä¾‹å¦‚ï¼š1115 Clement St, San Francisco, CAï¼‰",
    "",
    help="å¯ä»¥æ˜¯å®Œæ•´åœ°å€æˆ–è¡—é“ + åŸå¸‚ï¼Œç³»ç»Ÿä¼šç”¨ Google è‡ªåŠ¨åŒ¹é…é™„è¿‘çš„é¤å…ã€‚",
)

search_btn = st.button("ğŸ” æ ¹æ®åœ°å€æŸ¥æ‰¾é™„è¿‘é¤å…")

if search_btn:
    if not address_input.strip():
        st.error("è¯·å…ˆè¾“å…¥åœ°å€ã€‚")
    else:
        with st.spinner("æ ¹æ®åœ°å€å®šä½å¹¶æŸ¥æ‰¾é™„è¿‘é¤å…..."):
            geocode_res = google_geocode(GOOGLE_API_KEY, address_input)
            if not geocode_res:
                st.error("æ— æ³•é€šè¿‡è¯¥åœ°å€æ‰¾åˆ°ä½ç½®ï¼Œè¯·æ£€æŸ¥æ‹¼å†™ã€‚")
            else:
                loc = geocode_res[0]["geometry"]["location"]
                lat = loc["lat"]
                lng = loc["lng"]
                nearby = google_places_nearby(
                    GOOGLE_API_KEY, lat, lng, radius_m=300, type_="restaurant"
                )
                if not nearby:
                    st.warning("é™„è¿‘ 300 ç±³å†…æœªæ‰¾åˆ°é¤å…ï¼Œè¯·å°è¯•è¾“å…¥æ›´ç²¾ç¡®çš„åœ°å€æˆ–æ”¾å¤§èŒƒå›´ã€‚")
                else:
                    st.session_state["candidate_places"] = nearby
                    st.success(f"å·²æ‰¾åˆ° {len(nearby)} å®¶é™„è¿‘é¤å…ï¼Œè¯·åœ¨ä¸‹æ–¹é€‰æ‹©ä½ çš„é¤å…ã€‚")

# =========================
# 2ï¸âƒ£ é€‰æ‹©é¤å… + ä¸šåŠ¡å‚æ•°
# =========================

candidate_places = st.session_state["candidate_places"]
selected_place_id: Optional[str] = None
place_label_list: List[str] = []

run_btn = False

if candidate_places:
    st.markdown("## 2ï¸âƒ£ é€‰æ‹©ä½ çš„é¤å… & å¡«å†™å…³é”®ä¸šåŠ¡å‚æ•°")

    for p in candidate_places:
        label = f"{p.get('name', 'Unnamed')} â€” {p.get('vicinity', '')}"
        place_label_list.append(label)

    selected_index = st.selectbox(
        "åœ¨é™„è¿‘é¤å…åˆ—è¡¨ä¸­é€‰æ‹©ä½ è¦åˆ†æçš„é‚£ä¸€å®¶ï¼š",
        options=list(range(len(place_label_list))),
        format_func=lambda i: place_label_list[i],
        index=st.session_state.get("selected_index", 0),
    )
    st.session_state["selected_index"] = selected_index
    selected_place_id = candidate_places[selected_index]["place_id"]

    col_aov1, col_aov2 = st.columns(2)
    with col_aov1:
        dine_in_aov = st.number_input(
            "å ‚é£Ÿå¹³å‡å®¢å•ä»·ï¼ˆUSDï¼‰",
            min_value=5.0,
            max_value=300.0,
            value=35.0,
            step=1.0,
        )
    with col_aov2:
        delivery_aov = st.number_input(
            "å¤–å–å¹³å‡å®¢å•ä»·ï¼ˆUSDï¼‰",
            min_value=5.0,
            max_value=300.0,
            value=45.0,
            step=1.0,
        )

    st.markdown("### å…³é”®è¯ & æœç´¢é‡ï¼ˆä¸æ‡‚å°±ç”¨é»˜è®¤å€¼ï¼‰")

    keywords_input = st.text_input(
        "æ ¸å¿ƒå…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰",
        "best chinese food, best asian food, best baked chicken",
        help="ç”¨äºä¼°ç®—ä½ åœ¨ Google æœ¬åœ°æœç´¢é‡Œçš„æœºä¼šã€‚ä¸æ‡‚å°±ç”¨é»˜è®¤å€¼ã€‚",
    )

    monthly_search_volume = st.number_input(
        "ä¼°ç®—æ¯ä¸ªæ ¸å¿ƒå…³é”®è¯çš„æœˆæœç´¢é‡ï¼ˆç»Ÿä¸€ç²—ç•¥å€¼ï¼‰",
        min_value=50,
        max_value=50000,
        value=500,
        step=50,
        help="ç®€å•ç†è§£ä¸ºï¼šè¿™ä¸€ç±»å…³é”®è¯å¤§æ¦‚æ¯æœˆæœ‰å¤šå°‘äººæœç´¢ã€‚",
    )

    website_override = st.text_input(
        "å¦‚æœä½ çš„å®˜ç½‘å’Œ Google é‡Œè®°å½•çš„ä¸ä¸€æ ·ï¼Œåœ¨è¿™é‡Œå¡«ä½ çš„å®˜ç½‘ URLï¼ˆå¯é€‰ï¼‰",
        "",
    )

    run_btn = st.button("ğŸš€ è¿è¡Œåˆ†æ")

    if run_btn:
        st.session_state["analysis_ready"] = True
else:
    st.info("å…ˆè¾“å…¥åœ°å€å¹¶ç‚¹å‡»â€œæ ¹æ®åœ°å€æŸ¥æ‰¾é™„è¿‘é¤å…â€ã€‚")

# =========================
# 3ï¸âƒ£ ä¸»åˆ†æé€»è¾‘
# =========================

if candidate_places and selected_place_id and (
    run_btn or st.session_state.get("analysis_ready", False)
):
    with st.spinner("è·å–é¤å…è¯¦æƒ…ï¼ˆGoogle Place Detailsï¼‰..."):
        place_detail = google_place_details(GOOGLE_API_KEY, selected_place_id)

    st.success(f"å·²é”å®šé¤å…ï¼š**{place_detail.get('name', 'Unknown')}**")

    geometry = place_detail.get("geometry", {})
    location = geometry.get("location", {})
    center_lat = location.get("lat")
    center_lng = location.get("lng")

    with st.spinner("æ‰«æé™„è¿‘ 1.5 å…¬é‡Œå†…çš„ç«äº‰å¯¹æ‰‹..."):
        nearby_comp = google_places_nearby(
            GOOGLE_API_KEY, center_lat, center_lng, radius_m=1500, type_="restaurant"
        )

    competitors_rows = []
    for r in nearby_comp:
        pid = r.get("place_id")
        if pid == selected_place_id:
            continue
        competitors_rows.append(
            {
                "name": r.get("name"),
                "vicinity": r.get("vicinity"),
                "rating": r.get("rating"),
                "reviews": r.get("user_ratings_total"),
                "place_id": pid,
            }
        )

    competitors_df = pd.DataFrame(competitors_rows).sort_values(
        by=["rating", "reviews"], ascending=[False, False]
    )

    gbp_result = score_gbp_profile(place_detail)

    website_url = website_override.strip() or place_detail.get("website", "")
    website_html = None
    if website_url:
        with st.spinner("æŠ“å–å®˜ç½‘é¡µé¢ç”¨äºåˆ†æ..."):
            website_html = fetch_html(website_url)

    web_result = score_website_basic(website_url, website_html)

    st.markdown("## 3ï¸âƒ£ å…³é”®è¯æ’å & æ½œåœ¨è¥æ”¶æŸå¤±ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰")

    kw_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
    rank_rows: List[Dict[str, Any]] = []

    if SERPAPI_KEY and center_lat and center_lng:
        with st.spinner("é€šè¿‡ SerpAPI æŸ¥è¯¢ Google Maps æ’å..."):
            for kw in kw_list:
                try:
                    serp_json = serpapi_google_maps_search(
                        SERPAPI_KEY, kw, center_lat, center_lng
                    )
                    rank = infer_rank_from_serpapi(serp_json, place_detail.get("name", ""))
                except Exception:
                    rank = None

                if rank is None:
                    bucket = "none"
                elif rank <= 3:
                    bucket = "top3"
                elif rank <= 10:
                    bucket = "4-10"
                else:
                    bucket = "11+"

                dine_loss = estimate_revenue_loss(
                    monthly_search_volume, bucket, dine_in_aov, channel="dine-in"
                )
                delivery_loss = estimate_revenue_loss(
                    monthly_search_volume, bucket, delivery_aov, channel="delivery"
                )

                rank_rows.append(
                    {
                        "å…³é”®è¯": kw,
                        "é¢„ä¼°åæ¬¡": rank,
                        "åæ¬¡åŒºé—´": bucket,
                        "å ‚é£ŸæœˆæŸå¤±($)": round(dine_loss, 1),
                        "å¤–å–æœˆæŸå¤±($)": round(delivery_loss, 1),
                    }
                )
    else:
        st.warning("æœªé…ç½® SERPAPI_KEYï¼Œæ— æ³•è‡ªåŠ¨æŸ¥è¯¢ Google Maps æ’åï¼Œä»…å±•ç¤ºå…³é”®è¯åˆ—è¡¨ã€‚")
        for kw in kw_list:
            rank_rows.append(
                {
                    "å…³é”®è¯": kw,
                    "é¢„ä¼°åæ¬¡": None,
                    "åæ¬¡åŒºé—´": "unknown",
                    "å ‚é£ŸæœˆæŸå¤±($)": None,
                    "å¤–å–æœˆæŸå¤±($)": None,
                }
            )

    rank_df = pd.DataFrame(rank_rows)
    st.dataframe(rank_df, use_container_width=True)

    st.markdown("## 4ï¸âƒ£ Google å•†å®¶èµ„æ–™å¥åº·çŠ¶å†µï¼ˆProfileï¼‰")

    st.write(f"**Profile è¯„åˆ†ï¼š{gbp_result['score']} / 40**")
    gbp_checks_df = pd.DataFrame(
        [
            {"æ£€æŸ¥é¡¹": name, "å¾—åˆ†": pts, "æ˜¯å¦è¾¾æ ‡": "âœ… æ˜¯" if ok else "âŒ å¦"}
            for name, (pts, ok) in gbp_result["checks"].items()
        ]
    )
    st.dataframe(gbp_checks_df, use_container_width=True)

    st.markdown("## 5ï¸âƒ£ å®˜ç½‘å†…å®¹ & ç»“æ„å¥åº·çŠ¶å†µï¼ˆWebsiteï¼‰")

    st.write(f"**ç½‘ç«™è¯„åˆ†ï¼š{web_result['score']} / 40**")
    web_checks_df = pd.DataFrame(
        [
            {"æ£€æŸ¥é¡¹": name, "å¾—åˆ†": pts, "æ˜¯å¦è¾¾æ ‡": "âœ… æ˜¯" if ok else "âŒ å¦"}
            for name, (pts, ok) in web_result["checks"].items()
        ]
    )
    st.dataframe(web_checks_df, use_container_width=True)

    if website_url:
        st.write(f"å®˜ç½‘ï¼š{website_url}")
    else:
        st.warning("æœªåœ¨ Google èµ„æ–™ä¸­å‘ç°å®˜ç½‘é“¾æ¥ï¼Œç½‘ç«™è¯„åˆ†ä¼šåä½ã€‚")

    st.markdown("## 6ï¸âƒ£ é™„è¿‘ç«äº‰å¯¹æ‰‹æ¦‚è§ˆ")

    if not competitors_df.empty:
        st.dataframe(
            competitors_df[["name", "vicinity", "rating", "reviews"]],
            use_container_width=True,
        )
    else:
        st.info("æœªèƒ½æ‰¾åˆ°è¶³å¤Ÿçš„ç«äº‰å¯¹æ‰‹æ•°æ®ã€‚")

    st.markdown("## 7ï¸âƒ£ æ€»ä½“åœ¨çº¿å¥åº·æ€»ç»“")

    total_score = gbp_result["score"] + web_result["score"]
    st.write(f"**ç»¼åˆå¾—åˆ†ï¼ˆProfile + Websiteï¼‰ï¼š{total_score} / 80**")

    st.write(
        "- 40 åˆ†ä»¥ä¸‹ï¼šåœ¨çº¿åŸºç¡€éå¸¸è–„å¼±ï¼ŒåŸºæœ¬å±äº â€œPoorâ€ã€‚\n"
        "- 40â€“60 åˆ†ï¼šä¸­ç­‰ï¼Œèƒ½è¢«æ‰¾åˆ°ï¼Œä½†ä¸å ä¼˜åŠ¿ã€‚\n"
        "- 60 åˆ†ä»¥ä¸Šï¼šç›¸å¯¹å¥åº·ï¼Œå¯ä»¥å¼€å§‹ç©ç²¾ç»†åŒ–è¿è¥å’Œæ´»åŠ¨ã€‚"
    )

    # =============================
    # 8ï¸âƒ£ Google èœå•å›¾ç‰‡ â†’ è‡ªåŠ¨ OCR
    # =============================
    st.markdown("## 8ï¸âƒ£ Google èœå•å›¾ç‰‡ â†’ è‡ªåŠ¨ OCR æå–èœå“åŠä»·æ ¼ï¼ˆå¯é€‰ï¼‰")

    menu_photos = get_place_photos(place_detail, max_photos=20)

    if not menu_photos:
        st.info("æ²¡æœ‰ä» Google å›¾ç‰‡ä¸­è‡ªåŠ¨è¯†åˆ«å‡ºèœå•é¡µï¼Œå°†è·³è¿‡å›¾ç‰‡ OCRã€‚")
    else:
        st.write(f"å·²ä» Google å›¾ç‰‡ä¸­è‡ªåŠ¨è¯†åˆ«å‡º {len(menu_photos)} å¼ å¯èƒ½æ˜¯èœå•é¡µçš„å›¾ç‰‡ï¼š")
        cols = st.columns(4)
        for i, item in enumerate(menu_photos):
            with cols[i % 4]:
                st.image(item["image_bytes"], use_column_width=True)

        auto_ocr_btn = st.button("ğŸ§¾ è‡ªåŠ¨å¯¹èœå•é¡µåš OCR å¹¶æå–èœå•æ–‡æœ¬")

        if auto_ocr_btn:
            if client is None:
                st.error("æœªé…ç½® OPENAI_API_KEYï¼Œæ— æ³•è¿›è¡Œ OCRã€‚")
            else:
                ocr_results = []
                with st.spinner("AI æ­£åœ¨è¯†åˆ«èœå•é¡µä¸­çš„èœåå’Œä»·æ ¼â€¦"):
                    for item in menu_photos:
                        text = ocr_menu_from_image_bytes(item["image_bytes"])
                        if text:
                            ocr_results.append(text)

                if ocr_results:
                    st.session_state["ocr_menu_texts"] = ocr_results
                    st.success(f"ä»èœå•é¡µå›¾ç‰‡ä¸­æå–å‡º {len(ocr_results)} æ®µèœå•æ–‡æœ¬ã€‚")
                    for idx, txt in enumerate(ocr_results, start=1):
                        st.markdown(f"**OCR èœå• #{idx}ï¼š**")
                        st.code(txt, language="text")
                else:
                    st.warning("è‡ªåŠ¨è¯†åˆ«çš„èœå•é¡µä¸­æ²¡æœ‰æå–å‡ºæœ‰æ•ˆèœå•æ–‡æœ¬ã€‚")

    # =============================
    # 9ï¸âƒ£ èœå•æŠ“å–ï¼ˆå®˜ç½‘/å¤–å–é“¾æ¥ï¼‰+ åˆå¹¶ OCR èœå•
    # =============================

    st.markdown("## 9ï¸âƒ£ èœå•æŠ“å– & AI èœç³» / èœå•ç»“æ„åˆ†æ")

    auto_menu_urls = discover_menu_urls(place_detail, website_html)
    auto_menu_urls_str = "\n".join(auto_menu_urls)

    st.markdown("#### èœå•é“¾æ¥æŠ“å–ï¼ˆå¯æ‰‹åŠ¨å¢åˆ ï¼‰")
    menu_urls_input = st.text_area(
        "ç³»ç»Ÿè‡ªåŠ¨å‘ç°çš„èœå•/ç‚¹é¤é“¾æ¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œå¯è‡ªè¡Œå¢åˆ ï¼‰",
        auto_menu_urls_str,
        height=140,
    )

    menu_urls = [u.strip() for u in menu_urls_input.splitlines() if u.strip()]
    menus_payload: List[Dict[str, str]] = []

    if menu_urls:
        with st.spinner("å°è¯•æŠ“å–èœå•æ–‡æœ¬ï¼ˆå®˜ç½‘ / å¤–å–å¹³å°ï¼‰..."):
            menus_payload = build_menu_payload(menu_urls)

        if menus_payload:
            menu_preview_df = pd.DataFrame(
                [
                    {
                        "æ¥æº": m["source"],
                        "URL": m["url"],
                        "çŠ¶æ€": m["status"],
                        "èœå•æ–‡æœ¬é¢„è§ˆ": (m["menu_text"] or "")[:120].replace("\n", " "),
                    }
                    for m in menus_payload
                ]
            )
            st.dataframe(menu_preview_df, use_container_width=True)
    else:
        st.info("å½“å‰æ²¡æœ‰å¯ç”¨çš„èœå•é“¾æ¥ï¼ŒAI åˆ†æå°†ä¸»è¦åŸºäº Google èµ„æ–™å’Œå®˜ç½‘å†…å®¹ã€‚")

    # æŠŠ OCR å‡ºæ¥çš„èœå•æ–‡æœ¬ä¹Ÿå¡è¿› menus_payloadï¼ˆä½œä¸ºé¢å¤–æ¥æºï¼‰
    ocr_texts = st.session_state.get("ocr_menu_texts", [])
    for idx, txt in enumerate(ocr_texts, start=1):
        menus_payload.append(
            {
                "source": f"google_menu_photo_{idx}",
                "url": "",
                "status": "ocr_ok",
                "menu_text": txt,
            }
        )

    # ========== åŸºäºèœå•èœç³»ç”»åƒçš„ç²¾å‡†ç«å¯¹æ¨¡å— ==========
    st.markdown("### ğŸœ åŸºäºèœå•èœç³»ç”»åƒçš„ç²¾å‡†ç«å¯¹ï¼ˆå®éªŒåŠŸèƒ½ï¼‰")

    ai_comp_btn = st.button("âœ¨ ç”Ÿæˆèœç³»ç”»åƒ + ç²¾å‡†ç«å¯¹åˆ—è¡¨")

    if ai_comp_btn:
        if client is None:
            st.error("æœªé…ç½® OPENAI_API_KEYï¼Œæ— æ³•è¿›è¡Œèœç³»ç”»åƒå’Œç«å¯¹ç­›é€‰ã€‚")
        else:
            combined_menu_text_parts = [m["menu_text"] for m in menus_payload if m.get("menu_text")]
            combined_menu_text = "\n".join(combined_menu_text_parts)

            if not combined_menu_text.strip():
                st.warning("å½“å‰æœªèƒ½æˆåŠŸè·å–ä»»ä½•èœå•æ–‡æœ¬ï¼Œæ— æ³•è¿›è¡Œèœç³»ç”»åƒã€‚è¯·æ£€æŸ¥èœå•é“¾æ¥æˆ–èœå•å›¾ç‰‡ OCRã€‚")
            else:
                with st.spinner("AI æ­£åœ¨æ ¹æ®èœå•ç”Ÿæˆèœç³»ç”»åƒâ€¦"):
                    profile = analyze_menu_profile(combined_menu_text)

                if "error" in profile:
                    st.error(profile["error"])
                else:
                    st.subheader("ğŸ” AI èœç³»ç”»åƒ")
                    st.json(profile)

                    if competitors_df is None or competitors_df.empty:
                        st.info("é™„è¿‘ç«äº‰å¯¹æ‰‹æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›ä¸€æ­¥ç­›é€‰çœŸæ­£ç«å¯¹ã€‚")
                    else:
                        with st.spinner("AI æ­£åœ¨åŸºäºèœç³»ç”»åƒç­›é€‰çœŸæ­£çš„æ ¸å¿ƒç«å¯¹â€¦"):
                            candidate_profiles = build_competitor_profiles(
                                competitors_df, GOOGLE_API_KEY, max_n=15
                            )
                            ranked_competitors = rank_competitors_with_gpt(
                                profile, candidate_profiles
                            )

                        if not ranked_competitors:
                            st.warning("AI æœªèƒ½è¿”å›æœ‰æ•ˆçš„ç«å¯¹åˆ—è¡¨ï¼Œå¯èƒ½æ˜¯ä¿¡æ¯å¤ªå°‘æˆ–æ¨¡å‹è°ƒç”¨å‡ºé”™ã€‚")
                        else:
                            st.subheader("ğŸ† AI åˆ¤å®šçš„æ ¸å¿ƒç«å¯¹ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰")
                            ranked_df = pd.DataFrame(ranked_competitors)
                            st.dataframe(ranked_df, use_container_width=True)

    st.markdown("### ğŸ§  ç”Ÿæˆ ChatGPT èœç³» & èœå• & è¿è¥æ·±åº¦åˆ†ææŠ¥å‘Š")

    ai_btn = st.button("ğŸ“Š ç”Ÿæˆ AI æ·±åº¦åˆ†ææŠ¥å‘Šï¼ˆé•¿æ–‡ç‰ˆï¼‰")

    if ai_btn:
        st.info("å·²æ”¶åˆ°ç”Ÿæˆè¯·æ±‚ï¼Œæ­£åœ¨è°ƒç”¨ ChatGPT ...")

        if client is None:
            st.error("å½“å‰æœªé…ç½® OPENAI_API_KEYï¼Œæ— æ³•è°ƒç”¨ ChatGPTï¼Œè¯·åœ¨ Streamlit Secrets ä¸­æ·»åŠ  OPENAI_API_KEYã€‚")
        else:
            import traceback

            with st.spinner("æ­£åœ¨è°ƒç”¨ ChatGPT ç”Ÿæˆåˆ†ææŠ¥å‘Šâ€¦"):
                try:
                    ai_report = llm_deep_analysis(
                        place_detail=place_detail,
                        gbp_result=gbp_result,
                        web_result=web_result,
                        competitors_df=competitors_df,
                        rank_results=rank_rows,
                        monthly_search_volume=monthly_search_volume,
                        dine_in_aov=dine_in_aov,
                        delivery_aov=delivery_aov,
                        menus_payload=menus_payload,
                    )
                    st.markdown(ai_report)
                except Exception as e:
                    st.error(f"è°ƒç”¨ ChatGPT æ—¶å‘ç”Ÿæœªæ•è·é”™è¯¯ï¼š{e}")
                    st.code(traceback.format_exc())

    st.markdown("## ğŸ”Ÿ å…è´¹è·å–å®Œæ•´è¯Šæ–­æŠ¥å‘Š & 1 å¯¹ 1 å’¨è¯¢")

    st.markdown(
        """
        <a href="https://wa.me/6289995610" target="_blank"
           style="
             display:inline-block;
             padding:12px 24px;
             background:#25D366;
             color:#ffffff;
             border-radius:8px;
             text-decoration:none;
             font-weight:600;
             font-size:16px;
             margin-top:8px;
           ">
           ğŸ“² å…è´¹è·å–å®Œæ•´è¯Šæ–­æŠ¥å‘Šï¼ˆWhatsAppï¼‰
        </a>
        """,
        unsafe_allow_html=True,
    )

# ========== ç½²åï¼ˆLinkedInï¼‰ ==========
LINKEDIN_URL = "https://www.linkedin.com/in/lingyu-maxwell-lai"

st.markdown(
    f"""
<div style="display:flex;align-items:center;gap:10px;margin-top:18px;margin-bottom:8px;">
  <div style="font-size:14px;color:#666;">
    Builded by <strong>Maxwell Lai</strong>
  </div>
  <a href="{LINKEDIN_URL}" target="_blank" title="LinkedIn: Maxwell Lai"
     style="display:inline-flex;align-items:center;justify-content:center;
            width:18px;height:18px;border-radius:4px;background:#0A66C2;">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/linkedin.svg"
         alt="LinkedIn" width="12" height="12" style="filter: invert(1);" />
  </a>
</div>
""",
    unsafe_allow_html=True,
)
